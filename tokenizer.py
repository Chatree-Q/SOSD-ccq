import json
import os
import re
import regex # ä½¿ç”¨ç¬¬ä¸‰æ–¹regexåº“ä»¥æ›´å¥½åœ°æ”¯æŒ\p{L}ç­‰Unicodeå±æ€§
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
from functools import partial
from typing import Dict, List, Tuple, Optional, Iterable, Iterator
from collections import defaultdict 

# --- æ ¸å¿ƒè¾…åŠ©å‡½æ•° ---

def bytes_to_unicode():
    """å¤ç°GPT-2çš„bytes_to_unicodeæ˜ å°„é€»è¾‘"""
    # æ­¥éª¤1ï¼šæ”¶é›†å¯æ‰“å°ASCIIå­—ç¬¦ï¼ˆ33-126ï¼‰
    chars = [chr(i) for i in range(33, 127)]
    # æ­¥éª¤2ï¼šè¡¥å……ASCIIä¸­æœªåŒ…å«çš„å­—ç¬¦ï¼ˆ0-32ã€127ï¼‰
    chars += ['\u0000', '\u0001', ..., '\u001F', '\u007F']  # ç¤ºä¾‹ï¼Œéœ€å®Œæ•´è¦†ç›–
    # æ­¥éª¤3ï¼šå¤„ç†128-255å­—èŠ‚ï¼Œæ˜ å°„åˆ°Unicodeç§æœ‰åŒºåŸŸ
    chars += [chr(i + 0x100) for i in range(128)]  # ç¤ºä¾‹ï¼Œç¡®ä¿æ— å†²çª
    
    # ç”Ÿæˆæ˜ å°„ï¼šå­—èŠ‚å€¼ -> å¯¹åº”Unicodeå­—ç¬¦
    byte_to_char = {i: chars[i] for i in range(256)}
    return byte_to_char

def test_bytes_to_unicode_consistency():
    # åŠ è½½å‚è€ƒæ˜ å°„
    with open("bytes_to_unicode_reference.json", "r") as f:
        reference_mapping = json.load(f)
    # ç”Ÿæˆè‡ªå®šä¹‰æ˜ å°„
    custom_mapping = bytes_to_unicode()
    # è½¬æ¢ä¸ºç›¸åŒæ ¼å¼ï¼ˆå¦‚å­—èŠ‚å€¼ä¸ºé”®ï¼Œå­—ç¬¦ä¸ºå€¼ï¼‰
    reference = {int(k): v for k, v in reference_mapping.items()}
    # é€é”®å¯¹æ¯”
    assert custom_mapping == reference, "æ˜ å°„è¡¨ä¸å‚è€ƒä¸ä¸€è‡´"



# å…¨å±€æ˜ å°„è¡¨
byte_to_unicode = bytes_to_unicode()
unicode_to_byte = {v: k for k, v in byte_to_unicode.items()}

#def get_pair_stats_optimized(word_freqs: Dict[Tuple[int, ...], int]) -> Dict[Tuple[int, int], int]:
#    """
#    ä»è¯é¢‘å­—å…¸ä¸­é«˜æ•ˆåœ°è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡ã€‚
#    è¿™æ˜¯ä¼˜åŒ–çš„å…³é”®ï¼šæˆ‘ä»¬ä¸éå†æ•´ä¸ªæ–‡æœ¬ï¼Œè€Œæ˜¯éå†è¯æ±‡è¡¨å¹¶ä¹˜ä»¥å…¶é¢‘ç‡ã€‚
#    """
#    stats = {}
#    for word, freq in word_freqs.items():
#        for i in range(len(word) - 1):
#            pair = (word[i], word[i+1])
#            stats[pair] = stats.get(pair, 0) + freq
#    return stats
def init_pair_stats(word_freqs: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, str], int]:
    stats = defaultdict(int)
    for word, freq in word_freqs.items():
        for i in range(len(word)-1):
            stats[(word[i], word[i+1])] += freq
    return stats

def merge_word_freqs_optimized(word_freqs: Dict[Tuple[str, ...], int], pair: Tuple[str, str], new_char: str) -> Dict[Tuple[str, ...], int]:
    new_word_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        new_word = []
        i = 0
        n = len(word)
        while i < n:
            if i < n-1 and word[i] == pair[0] and word[i+1] == pair[1]:
                new_word.append(new_char)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        new_word_freqs[tuple(new_word)] += freq
    return new_word_freqs


def pretokenize_chunk(text_chunk: str, pat_str: str) -> Dict[Tuple[str, ...], int]:
    """å¹¶è¡ŒåŒ–é¢„åˆ†è¯çš„å·¥ä½œå‡½æ•°ï¼ˆè¿”å›Unicodeå­—ç¬¦tupleçš„è¯é¢‘ï¼‰"""
    pat = regex.compile(pat_str)
    word_freqs = defaultdict(int)  # æ”¹ä¸ºdefaultdict(int)
    for word_str in pat.findall(text_chunk):
        word_bytes = word_str.encode("utf-8")
        word_chars = tuple(byte_to_unicode[b] for b in word_bytes)
        word_freqs[word_chars] += 1
    return word_freqs


def update_pair_stats(word: Tuple[str, ...], old_pair: Tuple[str, str], new_char: str, stats: Dict[Tuple[str, str], int], freq: int):
    i = 0
    n = len(word)
    while i < n:
        if i < n-1 and word[i] == old_pair[0] and word[i+1] == old_pair[1]:
            # ç§»é™¤æ—§pair
            if stats[old_pair] >= freq:
                stats[old_pair] -= freq
                if stats[old_pair] == 0:
                    del stats[old_pair]
            # æ›´æ–°å·¦ä¾§æ–°pair
            if i > 0:
                left_pair = (word[i-1], new_char)
                stats[left_pair] += freq
                old_left_pair = (word[i-1], word[i])
                if stats[old_left_pair] >= freq:
                    stats[old_left_pair] -= freq
                    if stats[old_left_pair] == 0:
                        del stats[old_left_pair]
            # æ›´æ–°å³ä¾§æ–°pair
            if i < n-2:
                right_pair = (new_char, word[i+2])
                stats[right_pair] += freq
                old_right_pair = (word[i+1], word[i+2])
                if stats[old_right_pair] >= freq:
                    stats[old_right_pair] -= freq
                    if stats[old_right_pair] == 0:
                        del stats[old_right_pair]
            i += 2
        else:
            i += 1
            

# --- Problem 3: BPE è®­ç»ƒå‡½æ•° ---

# ä¿®å¤ 1: æŠŠå‡½æ•°å®šä¹‰æ”¾åœ¨ä¸€è¡Œï¼Œè§£å†³äº† SyntaxError
def train_bpe(input_path: str, vocab_size: int, special_tokens: List[str]) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:
    """
    è®­ç»ƒä¸€ä¸ªå­—èŠ‚çº§çš„BPEåˆ†è¯å™¨ã€‚

    Args:
        input_path: è®­ç»ƒæ•°æ®è·¯å¾„ã€‚
        vocab_size: ç›®æ ‡è¯æ±‡è¡¨å¤§å°ã€‚
        special_tokens: ç‰¹æ®Štokenåˆ—è¡¨ã€‚

    Returns:
        A tuple containing:
            - vocab: ä»token IDåˆ°å…¶å­—èŠ‚åºåˆ—çš„æ˜ å°„ã€‚
            - merges: æŒ‰åˆ›å»ºé¡ºåºåˆ—å‡ºçš„BPEåˆå¹¶è§„åˆ™ã€‚
    """

    # 1. è¯æ±‡è¡¨åˆå§‹åŒ– (Vocabulary initialization)
    vocab = {i: bytes([i]) for i in range(256)}
    next_token_id = 256
    for token_str in special_tokens:
        vocab[next_token_id] = token_str.encode("utf-8")
        next_token_id += 1


    # GPT-2çš„æ­£åˆ™è¡¨è¾¾å¼
    PAT_STR = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    
    # è¯»å–è¯­æ–™åº“
    with open(input_path, 'r', encoding='utf-8') as f:
        text = f.read()

    # 2. é¢„åˆ†è¯ (Pre-tokenization)
    # é¦–å…ˆæŒ‰ç‰¹æ®Štokenåˆ†å‰²è¯­æ–™åº“
    work_chunks = []
    if special_tokens:
        # æ„å»ºç‰¹æ®Štokençš„æ­£åˆ™ï¼ˆè½¬ä¹‰+æˆ–è¿æ¥ï¼‰
        special_pattern = "|".join(map(re.escape, special_tokens))
        # åˆ†å‰²æ–‡æœ¬ï¼šä¿ç•™ç‰¹æ®Štokenä½œä¸ºç‹¬ç«‹å—
        text_chunks = re.split(f"({special_pattern})", text)
        # è¿‡æ»¤ç©ºå—ï¼ŒåŒºåˆ†ç‰¹æ®Štokenå’Œæ™®é€šæ–‡æœ¬
        for chunk in text_chunks:
            if not chunk:
                continue
            if chunk in special_tokens:
                # ç‰¹æ®Štokenç›´æ¥ä½œä¸ºç‹¬ç«‹å—
                work_chunks.append(("special", chunk))
            else:
                # æ™®é€šæ–‡æœ¬ä½œä¸ºé¢„åˆ†è¯å—
                work_chunks.append(("text", chunk))
    else:
        # æ— ç‰¹æ®Štokenæ—¶ç›´æ¥ç”¨å…¨æ–‡
        work_chunks = [("text", text)]

    # å®šä¹‰å—å¤„ç†å‡½æ•°
    def process_chunk(chunk):
        chunk_type, chunk_content = chunk
        if chunk_type == "special":
            # ç‰¹æ®Štokenè½¬ä¸ºUnicodeå­—ç¬¦tupleï¼ˆä¸æ‹†åˆ†ï¼‰
            token_bytes = chunk_content.encode("utf-8")
            token_chars = tuple(byte_to_unicode[b] for b in token_bytes)
            return {token_chars: 1}
        else:
            # æ™®é€šæ–‡æœ¬é¢„åˆ†è¯
            return pretokenize_chunk(chunk_content, PAT_STR)

    # ä¿®å¤ 2: ä¿®æ­£äº† if/else çš„ç¼©è¿›é—®é¢˜
    # å¦‚æœæ•°æ®é‡å¾ˆå°ï¼ˆæ¯”å¦‚æµ‹è¯•ç”¨çš„ corpus.en åªæœ‰å‡ KBï¼‰ï¼Œå¼ºåˆ¶å•è¿›ç¨‹
    if len(text) >= 5_000_000: # å¤§æ–‡ä»¶æ‰ç”¨å¤šè¿›ç¨‹
        num_procs = min(cpu_count(), os.cpu_count() or 1)
        with Pool(num_procs) as pool:
            chunk_freqs_list = list(tqdm(
                pool.imap(process_chunk, work_chunks),
                total=len(work_chunks),
                desc="å¹¶è¡Œå¤„ç†å—"
            ))
    else: # å°æ–‡ä»¶å•è¿›ç¨‹
        chunk_freqs_list = [process_chunk(chunk) for chunk in work_chunks]

    
    # åˆå¹¶æ‰€æœ‰è¿›ç¨‹çš„ç»“æœ
    word_freqs = defaultdict(int)
    for chunk_freq in chunk_freqs_list:
        for word, freq in chunk_freq.items():
            word_freqs[word] += freq


    # 3. è®¡ç®— BPE åˆå¹¶ (Compute BPE merges)
    stats = init_pair_stats(word_freqs)
    merges_list = []
    num_merges = vocab_size - len(vocab)


    
    pbar = tqdm(range(num_merges), desc="BPE åˆå¹¶")
    for i in pbar:
        # (a) ç»Ÿè®¡æ‰€æœ‰ç›¸é‚» token å¯¹çš„é¢‘ç‡
        if not stats:
            print("æ²¡æœ‰æ›´å¤šçš„å¯¹å¯ä»¥åˆå¹¶ï¼Œæå‰åœæ­¢ã€‚")
            break

        # (b) æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„ token å¯¹ï¼Œå¹¶å¤„ç†å¹³å±€
        # ä¿®æ­£æ’åºè§„åˆ™ï¼šé¢‘ç‡é«˜ä¼˜å…ˆï¼Œé¢‘ç‡ç›¸åŒæŒ‰Unicodeå­—ç¬¦é¡ºåº
        best_pair = max(stats.items(), key=lambda x: (x[1], x[0]))[0]
        
         # ç”Ÿæˆæ–°çš„åˆå¹¶å­—ç¬¦ï¼ˆå¦‚'Ä t'ï¼‰
        new_char = best_pair[0] + best_pair[1]
         # bpe_merges[best_pair] = new_char

         #ï¼ˆcï¼‰ å¢é‡æ›´æ–°word_freqså’Œstats
        word_freqs = merge_word_freqs_optimized(word_freqs, best_pair, new_char)
         # ========== æ–°å¢ï¼šå¢é‡æ›´æ–°statsï¼ˆåªæ›´æ–°å—å½±å“çš„pairï¼‰ ==========
        
        for word, freq in word_freqs.items():
            update_pair_stats(word, best_pair, new_char, stats, freq)


        # (d) å°† "AB" æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­
        p1_bytes = best_pair[0].encode('utf-8')  # æ­£ç¡®è·å–å­—èŠ‚åºåˆ—
        p2_bytes = best_pair[1].encode('utf-8')
        merges_list.append((p1_bytes, p2_bytes))


        # (e) å°† ("A", "B") è®°å½•åˆ°åˆå¹¶è§„åˆ™åˆ—è¡¨ merges ä¸­
         # æ›´æ–°vocabï¼ˆnew_charè½¬å›bytesï¼‰
        new_char_bytes = bytes([unicode_to_byte[c] for c in new_char])
        vocab[next_token_id] = new_char_bytes
        
        next_token_id += 1

    return vocab, merges_list
   


# --- Problem 5: Tokenizer ç±»å®ç° (å·²ä¿®æ­£) ---

class BPE_Tokenizer:
    def __init__(self, vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], special_tokens: Optional[List[str]] = None):
        self.vocab = vocab
        # å°† merges è½¬ä¸ºå­—å…¸ï¼Œå€¼ä¸ºä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šä¼˜å…ˆï¼‰
        # é”®æ˜¯ (bytes, bytes)
        self.merges = {tuple(pair): i for i, pair in enumerate(merges)} 
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        PAT_STR = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
        self.pat = regex.compile(PAT_STR)

        # æ„å»ºç¼–ç å™¨ï¼šbytes -> ID
        self.encoder = {v: k for k, v in vocab.items()}
        # æ„å»ºè§£ç å™¨ï¼šID -> bytes
        self.decoder = vocab

        # å¤„ç†ç‰¹æ®Štoken
        self.special_tokens = set(special_tokens) if special_tokens else set()
        self.special_pattern = None
        self.special_encoder = {}
        if self.special_tokens:
            sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)
            pattern_str = "|".join(map(re.escape,sorted_special_tokens))
            self.special_pattern = regex.compile(f"({pattern_str})")
            # å»ºç«‹ç‰¹æ®Štokençš„å­—ç¬¦ä¸²åˆ°IDçš„æ˜ å°„
            for token_str in self.special_tokens:
                token_bytes = token_str.encode("utf-8")
                if token_bytes in self.encoder:
                    self.special_encoder[token_str] = self.encoder[token_bytes]

        # ç¼“å­˜
        self.cache = {}

    @classmethod
    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: Optional[List[str]] = None):
        with open(vocab_filepath, 'r', encoding='utf-8') as f:
            vocab_json = json.load(f)
            # å…³é”®ï¼šåŠ è½½ JSON æ—¶ï¼ŒKey æ˜¯å­—ç¬¦ä¸²ï¼Œå¿…é¡»è½¬ intï¼›Value æ˜¯å­—ç¬¦ä¸²ï¼Œå¿…é¡»è½¬ bytes
            # æ³¨æ„ï¼šè¿™é‡Œçš„ decode('unicode_escape').encode('latin1') æ˜¯ä¸ºäº†è¿˜åŸè¢« json åºåˆ—åŒ–æ—¶çš„å­—èŠ‚
            vocab = {}
            for k, v in vocab_json.items():
                vocab[int(k)] = v.encode('utf-8').decode('unicode_escape').encode('latin1')

        merges = []
        with open(merges_filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line: continue
                # mergesæ–‡ä»¶é€šå¸¸æ˜¯ "tokenA tokenB"
                parts = line.split()
                if len(parts) != 2: continue # è·³è¿‡ç©ºè¡Œæˆ–æ ¼å¼é”™è¯¯çš„è¡Œ
                p1, p2 = parts
                p1_bytes = p1.encode('utf-8').decode('unicode_escape').encode('latin1')
                p2_bytes = p2.encode('utf-8').decode('unicode_escape').encode('latin1')
                merges.append((p1_bytes, p2_bytes))
        
        return cls(vocab, merges, special_tokens)

    def save(self, vocab_filepath: str, merges_filepath: str):
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_json_save = {k: v.decode('latin1').encode('unicode_escape').decode('utf-8') for k, v in self.vocab.items()}
        with open(vocab_filepath, 'w', encoding='utf-8') as f:
            json.dump(vocab_json_save, f, ensure_ascii=False, indent=2)

        # ä¿å­˜åˆå¹¶è§„åˆ™
        with open(merges_filepath, 'w', encoding='utf-8') as f:
            for p1, p2 in self.merges.keys():
                 p1_str = p1.decode('latin1').encode('unicode_escape').decode('utf-8')
                 p2_str = p2.decode('latin1').encode('unicode_escape').decode('utf-8')
                 f.write(f"{p1_str} {p2_str}\n")
    
    def _bpe_merge(self, word_bytes: bytes) -> List[int]:
        """
        å¯¹å•ä¸ªå•è¯è¿›è¡ŒBPEåˆå¹¶ã€‚
        æ³¨æ„ï¼šè¿™é‡Œæ“ä½œçš„æ˜¯ Token IDï¼Œè€Œä¸æ˜¯åŸå§‹å­—èŠ‚å€¼ã€‚
        """
        if word_bytes in self.cache:
            return self.cache[word_bytes]

        # 1. åˆå§‹æ­¥éª¤ï¼šå°†å­—èŠ‚åºåˆ—è½¬æ¢ä¸º ID åºåˆ—
        # ä¿®æ­£ä»£ç ï¼šæŸ¥ encoder è¡¨
        tokens = [self.encoder[bytes([b])] for b in word_bytes]

        while len(tokens) >= 2:
            # å¯»æ‰¾å½“å‰ tokens åˆ—è¡¨ä¸­æ‰€æœ‰ç›¸é‚»å¯¹ä¸­ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼ˆrankå€¼æœ€å°ï¼‰çš„ä¸€å¯¹
            stats = {}
            for i in range(len(tokens) - 1):
                # è·å–ç›¸é‚»ä¸¤ä¸ª ID å¯¹åº”çš„å­—èŠ‚åºåˆ—
                p1_bytes = self.decoder[tokens[i]]
                p2_bytes = self.decoder[tokens[i+1]]
                pair = (p1_bytes, p2_bytes)
                
                # å¦‚æœè¿™ä¸ªå¯¹åœ¨åˆå¹¶è§„åˆ™é‡Œï¼Œè®°å½•å®ƒçš„ä¼˜å…ˆçº§
                if pair in self.merges:
                    stats[pair] = self.merges[pair]

            # å¦‚æœæ²¡æœ‰å¯åˆå¹¶çš„å¯¹ï¼Œé€€å‡ºå¾ªç¯
            if not stats:
                break

            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜ï¼ˆæ•°å€¼æœ€å°ï¼‰çš„å¯¹
            best_pair = min(stats, key=stats.get)
            
            # è®¡ç®—åˆå¹¶åçš„æ–° Token çš„ ID
            # æ³¨æ„ï¼šåˆå¹¶åçš„ bytes = p1_bytes + p2_bytes
            merged_bytes = best_pair[0] + best_pair[1]
            new_id = self.encoder[merged_bytes]

            # æ‰§è¡Œåˆå¹¶ï¼šåœ¨ tokens åˆ—è¡¨ä¸­æ›¿æ¢æ‰æ‰€æœ‰çš„ best_pair
            new_tokens = []
            i = 0
            while i < len(tokens):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬è¦åˆå¹¶çš„å¯¹
                # éœ€è¦å†æ¬¡æŸ¥è¡¨ç¡®è®¤ current bytes æ˜¯å¦åŒ¹é…
                if i < len(tokens) - 1:
                    b1 = self.decoder[tokens[i]]
                    b2 = self.decoder[tokens[i+1]]
                    if (b1, b2) == best_pair:
                        new_tokens.append(new_id)
                        i += 2
                        continue
                
                new_tokens.append(tokens[i])
                i += 1
            
            tokens = new_tokens
        
        self.cache[word_bytes] = tokens
        return tokens

    def encode(self, text: str) -> List[int]:
        """å°†å­—ç¬¦ä¸²ç¼–ç ä¸º token ID åˆ—è¡¨"""
        token_ids = []
        
        # å¤„ç†ç‰¹æ®Š token
        if self.special_pattern:
            chunks = self.special_pattern.split(text)
            for i, chunk in enumerate(chunks):
                if i % 2 == 1: # ç‰¹æ®Š token
                    if chunk in self.special_encoder:
                        token_ids.append(self.special_encoder[chunk])
                    else:
                        print(f"Warning: Special token {chunk} not found in vocab.")
                else: # æ™®é€šæ–‡æœ¬
                    if chunk:
                        for word in self.pat.findall(chunk):
                            word_bytes = word.encode("utf-8")
                            token_ids.extend(self._bpe_merge(word_bytes))
        else:
            for word in self.pat.findall(text):
                word_bytes = word.encode("utf-8")
                token_ids.extend(self._bpe_merge(word_bytes))

        return token_ids

    # --- æ–°å¢çš„æ–¹æ³•ï¼šProblem 6 è¦æ±‚ ---
    def encode_iterable(self, text_iterable: Iterable[str]) -> Iterator[int]:
        """
        å¯¹ä¸€ä¸ªæ–‡æœ¬è¿­ä»£å™¨è¿›è¡Œç¼–ç ã€‚
        è¿™ç”¨äºå¤„ç†å¤§å‹æ•°æ®é›†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ–‡æœ¬ã€‚
        """
        for text in text_iterable:
            yield from self.encode(text) #è¿”å›æ•´æ•°ID

    def decode(self, ids: List[int]) -> str:
        """å°† token ID åˆ—è¡¨è§£ç ä¸ºå­—ç¬¦ä¸²"""
        # æ³¨æ„ï¼šä½¿ç”¨ self.decoder æŠŠ ID è½¬å› bytes
        # errors='replace' é˜²æ­¢éæ³•çš„ UTF-8 åºåˆ—å¯¼è‡´å´©æºƒ
        all_bytes = b"".join(self.decoder[i] for i in ids)
        text = all_bytes.decode("utf-8", errors='replace')
        return text

        
# --- ä¸»æ‰§è¡Œå— (ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º) ---
if __name__ == '__main__':
    # ä¿®å¤ 3: æ¢å¤äº†æ•°æ®ç”Ÿæˆä»£ç ï¼Œé¿å… FileNotFoundError
    import time
    import resource
    import os

    # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
    INPUT_PATH = "train_dummy.txt" 
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ‘ä»¬å°±ç°åœºé€ ä¸€ä¸ªï¼
    if not os.path.exists(INPUT_PATH):
        print(f"æ­£åœ¨ç”Ÿæˆæµ‹è¯•æ•°æ®åˆ° {INPUT_PATH} ...")
        with open(INPUT_PATH, "w", encoding="utf-8") as f:
            f.write("low low low low low\n")
            f.write("lower lower widest widest widest\n")
            f.write("newest newest newest newest newest newest\n")
            f.write("This is a simple test. Emoji: ğŸ˜Š. Chinese: è¿™é‡Œæœ‰ä¸€äº›ä¸­æ–‡æµ‹è¯•æ•°æ®ã€‚\n")
            f.write("The quick brown fox jumps over the lazy dog. " * 50)

    # è®­ç»ƒå‚æ•°
    VOCAB_SIZE = 500
    SPECIAL_TOKENS = ["<|endoftext|>"]
  

    # (a) è®­ç»ƒåˆ†è¯å™¨
    print("å¼€å§‹è®­ç»ƒBPEåˆ†è¯å™¨...")
    start_time = time.time()
    
    vocab, merges = train_bpe(INPUT_PATH, VOCAB_SIZE, SPECIAL_TOKENS)
    
    end_time = time.time()
    training_time = end_time - start_time
    
    # æŠ¥å‘Šè®­ç»ƒæ—¶é—´å’Œå†…å­˜å ç”¨
    memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024  # in MB
    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    print(f"å³°å€¼å†…å­˜å ç”¨: {memory_usage:.2f} MB")

    # è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token
    longest_token = max(vocab.values(), key=len)
    print(f"è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token (bytes): {longest_token}")
    print(f"è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token (str): '{longest_token.decode('utf-8', 'replace')}'")

    # ä¿å­˜è®­ç»ƒç»“æœ
    VOCAB_FILE = "tinystories_vocab.json"
    MERGES_FILE = "tinystories_merges.txt"
    tokenizer_for_saving = BPE_Tokenizer(vocab, merges, SPECIAL_TOKENS)
    tokenizer_for_saving.save(VOCAB_FILE, MERGES_FILE)
    print(f"è¯æ±‡è¡¨å·²ä¿å­˜åˆ° {VOCAB_FILE}")
    print(f"åˆå¹¶è§„åˆ™å·²ä¿å­˜åˆ° {MERGES_FILE}")
    
    # --- Problem 5 & 6: ä½¿ç”¨Tokenizerç±» ---
    print("\n--- Tokenizer å®éªŒ ---")
    
    # å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä»å†…å­˜åŠ è½½ï¼Œé¿å…ä¿å­˜/è¯»å–æ—¶çš„ç¼–ç é—®é¢˜
    tokenizer = BPE_Tokenizer(vocab, merges, SPECIAL_TOKENS)
    
    # æµ‹è¯•ç¼–ç å’Œè§£ç 
    text_to_test = "newest low lower ğŸ˜Šä½ å¥½<|endoftext|>"
    encoded = tokenizer.encode(text_to_test)
    decoded = tokenizer.decode(encoded)
    
    print(f"åŸå§‹æ–‡æœ¬: '{text_to_test}'")
    print(f"ç¼–ç ç»“æœ (token IDs): {encoded}")
    print(f"è§£ç ç»“æœ: '{decoded}'")
    
    if text_to_test == decoded:
        print("âœ… ç¼–ç  -> è§£ç  ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ è­¦å‘Šï¼šè§£ç ä¸åŒ¹é…")

    # (a) è®¡ç®—å‹ç¼©ç‡
    sample_text = "This is a sample document from TinyStories dataset to calculate the compression ratio."
    encoded_sample = tokenizer.encode(sample_text)
    num_bytes = len(sample_text.encode("utf-8"))
    num_tokens = len(encoded_sample)
    compression_ratio = num_bytes / num_tokens
    print(f"\n(a) å‹ç¼©ç‡ (bytes/token): {compression_ratio:.2f} ({num_bytes} bytes / {num_tokens} tokens)")

    # (b) ä¼°ç®—ååé‡
    large_text = sample_text * 1000
    start_time_enc = time.time()
    tokenizer.encode(large_text)
    end_time_enc = time.time()
    duration_enc = end_time_enc - start_time_enc
    if duration_enc > 0:
        throughput = len(large_text.encode("utf-8")) / duration_enc / 1e6 # MB/s
        print(f"(b) ç¼–ç ååé‡: {throughput:.2f} MB/s")
