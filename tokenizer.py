import json
import os
import re
import regex # ä½¿ç”¨ç¬¬ä¸‰æ–¹regexåº“ä»¥æ›´å¥½åœ°æ”¯æŒ\p{L}ç­‰Unicodeå±æ€§
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
from functools import partial
from typing import Dict, List, Tuple, Optional, Iterable, Iterator
from collections import defaultdict 

# --- æ ¸å¿ƒè¾…åŠ©å‡½æ•° ---

def bytes_to_unicode():
    """å¤ç°GPT-2çš„bytes_to_unicodeæ˜ å°„é€»è¾‘"""
   # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†å¯æ‰“å°ASCIIå­—ç¬¦ï¼ˆ33-126ï¼‰å’ŒLatin-1å¯æ‰“å°å­—ç¬¦ï¼ˆ161-255ï¼‰
    chars = []
    for i in range(ord('!'), ord('~') + 1):
        chars.append(chr(i))
    for i in range(ord('Â¡'), ord('Â¬') + 1):
        chars.append(chr(i))
    for i in range(ord('Â®'), ord('Ã¿') + 1):
        chars.append(chr(i))
    
    # ç¬¬äºŒæ­¥ï¼šè¡¥å……å‰©ä½™å­—ç¬¦ï¼ˆç”¨ç‰¹æ®Šç¬¦å·å¡«å……ï¼Œç¡®ä¿æ€»é•¿åº¦256ï¼‰
    n = 0
    while len(chars) < 256:
        if n not in chars:  # é¿å…é‡å¤
            chars.append(chr(n))
        n += 1
    
    # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ0-255åˆ°charsçš„æ˜ å°„
    byte_to_char = {i: chars[i] for i in range(256)}
    return byte_to_char

def test_bytes_to_unicode_consistency():
    # åŠ è½½å‚è€ƒæ˜ å°„
    with open("bytes_to_unicode_reference.json", "r") as f:
        reference_mapping = json.load(f)
    # ç”Ÿæˆè‡ªå®šä¹‰æ˜ å°„
    custom_mapping = bytes_to_unicode()
    # è½¬æ¢ä¸ºç›¸åŒæ ¼å¼ï¼ˆå¦‚å­—èŠ‚å€¼ä¸ºé”®ï¼Œå­—ç¬¦ä¸ºå€¼ï¼‰
    reference = {int(k): v for k, v in reference_mapping.items()}
    # é€é”®å¯¹æ¯”
    assert custom_mapping == reference, "æ˜ å°„è¡¨ä¸å‚è€ƒä¸ä¸€è‡´"



# å…¨å±€æ˜ å°„è¡¨
byte_to_unicode = bytes_to_unicode()
unicode_to_byte = {v: k for k, v in byte_to_unicode.items()}

def get_word_freqs(data: str) -> Dict[bytes, int]:
    """
    é¢„å¤„ç†æ–‡æœ¬ï¼Œè¿”å›å­—èŠ‚çº§åˆ«çš„è¯é¢‘ç»Ÿè®¡
    :param data: è¾“å…¥æ–‡æœ¬å­—ç¬¦ä¸²
    :return: {bytesè¯: é¢‘ç‡}
    """
    # å¤ç”¨BPE_Tokenizerçš„æ­£åˆ™æ¨¡å¼ï¼ˆéœ€ç¡®ä¿ä¸åˆ†è¯é€»è¾‘ä¸€è‡´ï¼‰
    PAT_STR = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    pat = regex.compile(PAT_STR)
    
    # 1. æŒ‰æ­£åˆ™åˆ‡åˆ†æ–‡æœ¬ä¸ºåŸºç¡€chunk
    chunks = pat.findall(data)
    # 2. è½¬ä¸ºå­—èŠ‚åºåˆ—ï¼Œç»Ÿè®¡è¯é¢‘
    word_freqs = {}
    for chunk in chunks:
        word_bytes = chunk.encode('utf-8')  # å­—ç¬¦ä¸²è½¬å­—èŠ‚
        word_freqs[word_bytes] = word_freqs.get(word_bytes, 0) + 1
    return word_freqs


def get_pair_freq(word_token_freqs: Dict[Tuple[int, ...], int]) -> Dict[Tuple[int, int], int]:
    """
    ç»Ÿè®¡tokenåºåˆ—ä¸­ç›¸é‚»å¯¹çš„é¢‘ç‡
    :param word_token_freqs: {(token_IDåºåˆ—,): é¢‘ç‡}
    :return: {(token1_ID, token2_ID): é¢‘ç‡}
    """
    pair_freq = {}
    for token_seq, freq in word_token_freqs.items():
        if len(token_seq) < 2:
            continue  # å•ä¸ªtokenæ— ç›¸é‚»å¯¹
        # éå†ç›¸é‚»tokenå¯¹
        for i in range(len(token_seq) - 1):
            pair = (token_seq[i], token_seq[i+1])
            pair_freq[pair] = pair_freq.get(pair, 0) + freq
    return pair_freq



def merge_tokens(word_token_freqs: Dict[Tuple[int, ...], int], 
                 best_pair: Tuple[int, int], 
                 new_id: int) -> Dict[Tuple[int, ...], int]:
    """
    å°†è¯åºåˆ—ä¸­çš„best_pairæ›¿æ¢ä¸ºnew_idï¼Œè¿”å›æ›´æ–°åçš„è¯é¢‘
    """
    new_word_token_freqs = {}
    for token_seq, freq in word_token_freqs.items():
        new_seq = []
        i = 0
        while i < len(token_seq):
            # åŒ¹é…åˆ°best_pairåˆ™æ›¿æ¢ä¸ºnew_idï¼Œè·³è¿‡ä¸‹ä¸€ä¸ªtoken
            if i < len(token_seq)-1 and (token_seq[i], token_seq[i+1]) == best_pair:
                new_seq.append(new_id)
                i += 2
            else:
                new_seq.append(token_seq[i])
                i += 1
        # æ›´æ–°è¯é¢‘ï¼ˆåˆå¹¶ç›¸åŒåºåˆ—ï¼‰
        new_seq_tuple = tuple(new_seq)
        new_word_token_freqs[new_seq_tuple] = new_word_token_freqs.get(new_seq_tuple, 0) + freq
    return new_word_token_freqs

            

# --- Problem 3: BPE è®­ç»ƒå‡½æ•° ---

def train_bpe(data: str, vocab_size: int, special_tokens: Optional[List[str]] = None) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:
    # 1. åˆå§‹åŒ–åŸºç¡€æ˜ å°„ï¼šå•å­—èŠ‚tokenï¼ˆ0-255ï¼‰
    encoder: Dict[bytes, int] = {bytes([i]): i for i in range(256)}
    decoder: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}
    
    
    # 2. å¤„ç†ç‰¹æ®Štokenï¼ˆæ·»åŠ åˆ°æ˜ å°„ä¸­ï¼‰
    if special_tokens:
        for token in special_tokens:
            token_bytes = token.encode('utf-8')
            if token_bytes not in encoder:
                new_id = len(encoder)
                encoder[token_bytes] = new_id
                decoder[new_id] = token_bytes
    
    # 3. é¢„å¤„ç†æ•°æ®ï¼šå¾—åˆ°è¯é¢‘ï¼ˆword: bytesï¼Œfreq: intï¼‰
    word_freqs = get_word_freqs(data)  # å‡è®¾ä½ æœ‰è¿™ä¸ªå‡½æ•°ï¼Œè¿”å›{bytes: int}
    
    # 4. å°†è¯ï¼ˆbytesï¼‰è½¬æ¢ä¸ºtoken IDåºåˆ—
    word_token_freqs = {}
    for word, freq in word_freqs.items():
        # æ¯ä¸ªå­—èŠ‚è½¬å¯¹åº”çš„IDï¼ˆä¾èµ–encoderï¼‰
        token_sequence = [encoder[bytes([b])] for b in word]
        word_token_freqs[tuple(token_sequence)] = freq
    
    # 5. åˆå§‹åŒ–åˆå¹¶è§„åˆ™å’Œç»Ÿè®¡
    merges: List[Tuple[bytes, bytes]] = []
    
    # 6. BPEåˆå¹¶å¾ªç¯ï¼ˆç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯æ±‡é‡ï¼‰
    while len(encoder) < vocab_size:
        pair_freq = get_pair_freq(word_token_freqs)
        if not pair_freq:
            break  # æ— æ›´å¤šå¯åˆå¹¶çš„å¯¹
        
        # æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„tokenå¯¹
        best_pair = max(pair_freq, key=pair_freq.get)
        p1_id, p2_id = best_pair
        
        # ä»decoderä¸­è·å–IDå¯¹åº”çš„å­—èŠ‚åºåˆ—
        p1_bytes = decoder[p1_id]
        p2_bytes = decoder[p2_id]
        merged_bytes = p1_bytes + p2_bytes
        
        # æ·»åŠ æ–°tokenåˆ°æ˜ å°„
        new_id = len(encoder)
        encoder[merged_bytes] = new_id
        decoder[new_id] = merged_bytes
        
        # æ›´æ–°åˆå¹¶è§„åˆ™
        merges.append((p1_bytes, p2_bytes))
        
        # æ›´æ–°è¯çš„tokenåºåˆ—å’Œé¢‘ç‡ç»Ÿè®¡
        word_token_freqs = merge_tokens(word_token_freqs, best_pair, new_id)
    
    # è¿”å›IDâ†’å­—èŠ‚çš„vocabå’Œåˆå¹¶è§„åˆ™
    return decoder, merges


   


# --- Problem 5: Tokenizer ç±»å®ç° (å·²ä¿®æ­£) ---

class BPE_Tokenizer:
    def __init__(self, vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], special_tokens: Optional[List[str]] = None):
        # å°† merges è½¬ä¸ºå­—å…¸ï¼Œå€¼ä¸ºä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šä¼˜å…ˆï¼‰
        # é”®æ˜¯ (bytes, bytes)
        self.merges = {pair: i for i, pair in enumerate(merges)}  
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        PAT_STR = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
        self.pat = regex.compile(PAT_STR)
        
        # æ„å»ºè§£ç å™¨ï¼šID -> bytes
        self.decoder = vocab
        # æ„å»ºç¼–ç å™¨ï¼šbytes -> ID
        self.encoder = {v: k for k, v in vocab.items()}
      

        # å¤„ç†ç‰¹æ®Štoken
        self.special_tokens = set(special_tokens) if special_tokens else set()
        self.special_pattern = None
        self.special_encoder = {}
        if self.special_tokens:
            sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)
            pattern_str = "|".join(map(re.escape,sorted_special_tokens))
            self.special_pattern = regex.compile(f"({pattern_str})")
            # å»ºç«‹ç‰¹æ®Štokençš„å­—ç¬¦ä¸²åˆ°IDçš„æ˜ å°„
            for token_str in self.special_tokens:
                token_bytes = token_str.encode("utf-8")
                if token_bytes in self.encoder:
                    self.special_encoder[token_str] = self.encoder[token_bytes]
                else:
                    # è¿™é‡Œæ’å…¥è­¦å‘Š/æŠ¥é”™é€»è¾‘ï¼ˆäºŒé€‰ä¸€å³å¯ï¼‰
                    # æŠ›å‡ºé”™è¯¯ï¼ˆæ¨èï¼Œå¼ºåˆ¶ç¡®ä¿ç‰¹æ®Štokenå­˜åœ¨ï¼‰
                    raise ValueError(f"Special token '{token_str}' (bytes: {token_bytes}) not found in vocab!")
  

        # ç¼“å­˜
        self.cache = {}

    @classmethod
    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: Optional[List[str]] = None):
        with open(vocab_filepath, 'r', encoding='utf-8') as f:
            vocab_json = json.load(f)
            # å…³é”®ï¼šåŠ è½½ JSON æ—¶ï¼ŒKey æ˜¯å­—ç¬¦ä¸²ï¼Œå¿…é¡»è½¬ intï¼›Value æ˜¯å­—ç¬¦ä¸²ï¼Œå¿…é¡»è½¬ bytes
            # æ³¨æ„ï¼šè¿™é‡Œçš„ decode('unicode_escape').encode('latin1') æ˜¯ä¸ºäº†è¿˜åŸè¢« json åºåˆ—åŒ–æ—¶çš„å­—èŠ‚
            vocab = {}
            for k, v in vocab_json.items():
                vocab[int(k)] = v.encode('utf-8').decode('unicode_escape').encode('latin1')

        merges = []
        with open(merges_filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line: continue
                # mergesæ–‡ä»¶é€šå¸¸æ˜¯ "tokenA tokenB"
                parts = line.split()
                if len(parts) != 2: continue # è·³è¿‡ç©ºè¡Œæˆ–æ ¼å¼é”™è¯¯çš„è¡Œ
                p1, p2 = parts
                p1_bytes = p1.encode('utf-8').decode('unicode_escape').encode('latin1')
                p2_bytes = p2.encode('utf-8').decode('unicode_escape').encode('latin1')
                merges.append((p1_bytes, p2_bytes))
        
        return cls(vocab, merges, special_tokens)

    def save(self, vocab_path: str, merges_path: str):
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_json_save = {k: v.decode('latin1').encode('unicode_escape').decode('utf-8') for k, v in self.vocab.items()}
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(vocab_json_save, f, ensure_ascii=False, indent=2)
        # ä¿å­˜åˆå¹¶è§„åˆ™
        merges_list = [list(pair) for pair in self.merges.keys()]
        with open(merges_path, 'w', encoding='utf-8') as f:
            json.dump(merges_list, f, indent=2)
            for p1, p2 in self.merges.keys():
                 p1_str = p1.decode('latin1').encode('unicode_escape').decode('utf-8')
                 p2_str = p2.decode('latin1').encode('unicode_escape').decode('utf-8')
                 f.write(f"{p1_str} {p2_str}\n")
    
    def _bpe_merge(self, word_bytes: bytes) -> List[int]:
        """
        å¯¹å•ä¸ªå•è¯è¿›è¡ŒBPEåˆå¹¶ã€‚
        æ³¨æ„ï¼šè¿™é‡Œæ“ä½œçš„æ˜¯ Token IDï¼Œè€Œä¸æ˜¯åŸå§‹å­—èŠ‚å€¼ã€‚
        """
        if word_bytes in self.cache:
            return self.cache[word_bytes]

        # 1. åˆå§‹æ­¥éª¤ï¼šå°†å­—èŠ‚åºåˆ—è½¬æ¢ä¸º ID åºåˆ—
        # ä¿®æ­£ä»£ç ï¼šæŸ¥ encoder è¡¨
        tokens = [self.encoder[bytes([b])] for b in word_bytes]

        while len(tokens) >= 2:
            # å¯»æ‰¾å½“å‰ tokens åˆ—è¡¨ä¸­æ‰€æœ‰ç›¸é‚»å¯¹ä¸­ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼ˆrankå€¼æœ€å°ï¼‰çš„ä¸€å¯¹
            stats = {}
            for i in range(len(tokens) - 1):
                # è·å–ç›¸é‚»ä¸¤ä¸ª ID å¯¹åº”çš„å­—èŠ‚åºåˆ—
                p1_bytes = self.decoder[tokens[i]]
                p2_bytes = self.decoder[tokens[i+1]]
                pair = (p1_bytes, p2_bytes)
                
                # å¦‚æœè¿™ä¸ªå¯¹åœ¨åˆå¹¶è§„åˆ™é‡Œï¼Œè®°å½•å®ƒçš„ä¼˜å…ˆçº§
                if pair in self.merges:
                    stats[pair] = self.merges[pair]

            # å¦‚æœæ²¡æœ‰å¯åˆå¹¶çš„å¯¹ï¼Œé€€å‡ºå¾ªç¯
            if not stats:
                break

            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜ï¼ˆæ•°å€¼æœ€å°ï¼‰çš„å¯¹
            best_pair = min(stats, key=stats.get)
            
            # è®¡ç®—åˆå¹¶åçš„æ–° Token çš„ ID
            # æ³¨æ„ï¼šåˆå¹¶åçš„ bytes = p1_bytes + p2_bytes
            merged_bytes = best_pair[0] + best_pair[1]
            new_id = self.encoder[merged_bytes]

            # æ‰§è¡Œåˆå¹¶ï¼šåœ¨ tokens åˆ—è¡¨ä¸­æ›¿æ¢æ‰æ‰€æœ‰çš„ best_pair
            new_tokens = []
            i = 0
            while i < len(tokens):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬è¦åˆå¹¶çš„å¯¹
                # éœ€è¦å†æ¬¡æŸ¥è¡¨ç¡®è®¤ current bytes æ˜¯å¦åŒ¹é…
                if i < len(tokens) - 1:
                    b1 = self.decoder[tokens[i]]
                    b2 = self.decoder[tokens[i+1]]
                    if (b1, b2) == best_pair:
                        new_tokens.append(new_id)
                        i += 2
                        continue
                
                new_tokens.append(tokens[i])
                i += 1
            
            tokens = new_tokens
        
        self.cache[word_bytes] = tokens
        return tokens

    def encode(self, text: str) -> List[int]:
        """å°†å­—ç¬¦ä¸²ç¼–ç ä¸º token ID åˆ—è¡¨"""
        token_ids = []
        
        # å¤„ç†ç‰¹æ®Š token
        if self.special_pattern:
            chunks = self.special_pattern.split(text)
            for i, chunk in enumerate(chunks):
                if i % 2 == 1: # ç‰¹æ®Š token
                    if chunk in self.special_encoder:
                        token_ids.append(self.special_encoder[chunk])
                    else:
                        print(f"Warning: Special token {chunk} not found in vocab.")
                else: # æ™®é€šæ–‡æœ¬
                    if chunk:
                        for word in self.pat.findall(chunk):
                            word_bytes = word.encode("utf-8")
                            token_ids.extend(self._bpe_merge(word_bytes))
        else:
            for word in self.pat.findall(text):
                word_bytes = word.encode("utf-8")
                token_ids.extend(self._bpe_merge(word_bytes))

        return token_ids

    # --- æ–°å¢çš„æ–¹æ³•ï¼šProblem 6 è¦æ±‚ ---
    def encode_iterable(self, text_iterable: Iterable[str]) -> Iterator[int]:
        """
        å¯¹ä¸€ä¸ªæ–‡æœ¬è¿­ä»£å™¨è¿›è¡Œç¼–ç ã€‚
        è¿™ç”¨äºå¤„ç†å¤§å‹æ•°æ®é›†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ–‡æœ¬ã€‚
        """
        for text in text_iterable:
            yield from self.encode(text) #è¿”å›æ•´æ•°ID

    def decode(self, ids: List[int]) -> str:
        """å°† token ID åˆ—è¡¨è§£ç ä¸ºå­—ç¬¦ä¸²"""
        # æ³¨æ„ï¼šä½¿ç”¨ self.decoder æŠŠ ID è½¬å› bytes
        # errors='replace' é˜²æ­¢éæ³•çš„ UTF-8 åºåˆ—å¯¼è‡´å´©æºƒ
        all_bytes = b"".join(self.decoder[i] for i in ids)
        text = all_bytes.decode("utf-8", errors='replace')
        return text

        
# --- ä¸»æ‰§è¡Œå— (ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º) ---
if __name__ == '__main__':
    # ä¿®å¤ 3: æ¢å¤äº†æ•°æ®ç”Ÿæˆä»£ç ï¼Œé¿å… FileNotFoundError
    import time
    import resource
    import os

    # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
    INPUT_PATH = "train_dummy.txt" 
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ‘ä»¬å°±ç°åœºé€ ä¸€ä¸ªï¼
    if not os.path.exists(INPUT_PATH):
        print(f"æ­£åœ¨ç”Ÿæˆæµ‹è¯•æ•°æ®åˆ° {INPUT_PATH} ...")
        with open(INPUT_PATH, "w", encoding="utf-8") as f:
            f.write("low low low low low\n")
            f.write("lower lower widest widest widest\n")
            f.write("newest newest newest newest newest newest\n")
            f.write("This is a simple test. Emoji: ğŸ˜Š. Chinese: è¿™é‡Œæœ‰ä¸€äº›ä¸­æ–‡æµ‹è¯•æ•°æ®ã€‚\n")
            f.write("The quick brown fox jumps over the lazy dog. " * 50)
        with open(INPUT_PATH, "r", encoding="utf-8") as f:
            data = f.read()  # è¯»å–æ–‡ä»¶å†…å®¹
        vocab, merges = train_bpe(data, VOCAB_SIZE, SPECIAL_TOKENS)  # ä¼ å…¥å†…å®¹è€Œéè·¯å¾„
        
    # è®­ç»ƒå‚æ•°
    VOCAB_SIZE = 500
    SPECIAL_TOKENS = ["<|endoftext|>"]
  

    # (a) è®­ç»ƒåˆ†è¯å™¨
    print("å¼€å§‹è®­ç»ƒBPEåˆ†è¯å™¨...")
    start_time = time.time()
    
    vocab, merges = train_bpe(data, VOCAB_SIZE, SPECIAL_TOKENS)
    
    end_time = time.time()
    training_time = end_time - start_time
    
    # æŠ¥å‘Šè®­ç»ƒæ—¶é—´å’Œå†…å­˜å ç”¨
    memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024  # in MB
    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    print(f"å³°å€¼å†…å­˜å ç”¨: {memory_usage:.2f} MB")

    # è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token
    longest_token = max(vocab.values(), key=len)
    print(f"è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token (bytes): {longest_token}")
    print(f"è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token (str): '{longest_token.decode('utf-8', 'replace')}'")

    # ä¿å­˜è®­ç»ƒç»“æœ
    VOCAB_FILE = "tinystories_vocab.json"
    MERGES_FILE = "tinystories_merges.txt"
    tokenizer_for_saving = BPE_Tokenizer(vocab, merges, SPECIAL_TOKENS)
    tokenizer_for_saving.save(VOCAB_FILE, MERGES_FILE)
    print(f"è¯æ±‡è¡¨å·²ä¿å­˜åˆ° {VOCAB_FILE}")
    print(f"åˆå¹¶è§„åˆ™å·²ä¿å­˜åˆ° {MERGES_FILE}")
    
    # --- Problem 5 & 6: ä½¿ç”¨Tokenizerç±» ---
    print("\n--- Tokenizer å®éªŒ ---")
    
    # å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä»å†…å­˜åŠ è½½ï¼Œé¿å…ä¿å­˜/è¯»å–æ—¶çš„ç¼–ç é—®é¢˜
    tokenizer = BPE_Tokenizer(vocab, merges, SPECIAL_TOKENS)
    
    # æµ‹è¯•ç¼–ç å’Œè§£ç 
    text_to_test = "newest low lower ğŸ˜Šä½ å¥½<|endoftext|>"
    encoded = tokenizer.encode(text_to_test)
    decoded = tokenizer.decode(encoded)
    
    print(f"åŸå§‹æ–‡æœ¬: '{text_to_test}'")
    print(f"ç¼–ç ç»“æœ (token IDs): {encoded}")
    print(f"è§£ç ç»“æœ: '{decoded}'")
    
    if text_to_test == decoded:
        print("âœ… ç¼–ç  -> è§£ç  ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ è­¦å‘Šï¼šè§£ç ä¸åŒ¹é…")

    # (a) è®¡ç®—å‹ç¼©ç‡
    sample_text = "This is a sample document from TinyStories dataset to calculate the compression ratio."
    encoded_sample = tokenizer.encode(sample_text)
    num_bytes = len(sample_text.encode("utf-8"))
    num_tokens = len(encoded_sample)
    compression_ratio = num_bytes / num_tokens
    print(f"\n(a) å‹ç¼©ç‡ (bytes/token): {compression_ratio:.2f} ({num_bytes} bytes / {num_tokens} tokens)")

    # (b) ä¼°ç®—ååé‡
    large_text = sample_text * 1000
    start_time_enc = time.time()
    tokenizer.encode(large_text)
    end_time_enc = time.time()
    duration_enc = end_time_enc - start_time_enc
    if duration_enc > 0:
        throughput = len(large_text.encode("utf-8")) / duration_enc / 1e6 # MB/s
        print(f"(b) ç¼–ç ååé‡: {throughput:.2f} MB/s")
