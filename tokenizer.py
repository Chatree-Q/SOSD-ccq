import json
import os
import re
import regex # ä½¿ç”¨ç¬¬ä¸‰æ–¹regexåº“ä»¥æ›´å¥½åœ°æ”¯æŒ\p{L}ç­‰Unicodeå±æ€§
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
from functools import partial
from typing import Dict, List, Tuple, Optional, Iterable, Iterator
from collections import defaultdict 

# --- æ ¸å¿ƒè¾…åŠ©å‡½æ•° ---

def bytes_to_unicode():
    """å¤ç°GPT-2çš„bytes_to_unicodeæ˜ å°„é€»è¾‘"""
   # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†å¯æ‰“å°ASCIIå­—ç¬¦ï¼ˆ33-126ï¼‰å’ŒLatin-1å¯æ‰“å°å­—ç¬¦ï¼ˆ161-255ï¼‰
    chars = []
    for i in range(ord('!'), ord('~') + 1):
        chars.append(chr(i))
    for i in range(ord('Â¡'), ord('Â¬') + 1):
        chars.append(chr(i))
    for i in range(ord('Â®'), ord('Ã¿') + 1):
        chars.append(chr(i))
    
    # ç¬¬äºŒæ­¥ï¼šè¡¥å……å‰©ä½™å­—ç¬¦ï¼ˆç”¨ç‰¹æ®Šç¬¦å·å¡«å……ï¼Œç¡®ä¿æ€»é•¿åº¦256ï¼‰
    n = 0
    while len(chars) < 256:
        if n not in chars:  # é¿å…é‡å¤
            chars.append(chr(n))
        n += 1
    
    # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ0-255åˆ°charsçš„æ˜ å°„
    byte_to_char = {i: chars[i] for i in range(256)}
    return byte_to_char

def test_bytes_to_unicode_consistency():
    # åŠ è½½å‚è€ƒæ˜ å°„
    with open("bytes_to_unicode_reference.json", "r") as f:
        reference_mapping = json.load(f)
    # ç”Ÿæˆè‡ªå®šä¹‰æ˜ å°„
    custom_mapping = bytes_to_unicode()
    # è½¬æ¢ä¸ºç›¸åŒæ ¼å¼ï¼ˆå¦‚å­—èŠ‚å€¼ä¸ºé”®ï¼Œå­—ç¬¦ä¸ºå€¼ï¼‰
    reference = {int(k): v for k, v in reference_mapping.items()}
    # é€é”®å¯¹æ¯”
    assert custom_mapping == reference, "æ˜ å°„è¡¨ä¸å‚è€ƒä¸ä¸€è‡´"



# å…¨å±€æ˜ å°„è¡¨
byte_to_unicode = bytes_to_unicode()
unicode_to_byte = {v: k for k, v in byte_to_unicode.items()}

#def get_pair_stats_optimized(word_freqs: Dict[Tuple[int, ...], int]) -> Dict[Tuple[int, int], int]:
#    """
#    ä»è¯é¢‘å­—å…¸ä¸­é«˜æ•ˆåœ°è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡ã€‚
#    è¿™æ˜¯ä¼˜åŒ–çš„å…³é”®ï¼šæˆ‘ä»¬ä¸éå†æ•´ä¸ªæ–‡æœ¬ï¼Œè€Œæ˜¯éå†è¯æ±‡è¡¨å¹¶ä¹˜ä»¥å…¶é¢‘ç‡ã€‚
#    """
#    stats = {}
#    for word, freq in word_freqs.items():
#        for i in range(len(word) - 1):
#            pair = (word[i], word[i+1])
#            stats[pair] = stats.get(pair, 0) + freq
#    return stats
def init_pair_stats(word_freqs: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, str], int]:
    stats = defaultdict(int)
    for word, freq in word_freqs.items():
        for i in range(len(word)-1):
            stats[(word[i], word[i+1])] += freq
    return stats

def merge_word_freqs_optimized(word_freqs: Dict[Tuple[str, ...], int], pair: Tuple[str, str], new_char: str) -> Dict[Tuple[str, ...], int]:
    new_word_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        new_word = []
        i = 0
        n = len(word)
        while i < n:
            if i < n-1 and word[i] == pair[0] and word[i+1] == pair[1]:
                new_word.append(new_char)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        new_word_freqs[tuple(new_word)] += freq
    return new_word_freqs


def pretokenize_chunk(text_chunk: str, pat_str: str) -> Dict[Tuple[str, ...], int]:
    """å¹¶è¡ŒåŒ–é¢„åˆ†è¯çš„å·¥ä½œå‡½æ•°ï¼ˆè¿”å›Unicodeå­—ç¬¦tupleçš„è¯é¢‘ï¼‰"""
    pat = regex.compile(pat_str)
    word_freqs = defaultdict(int)  # æ”¹ä¸ºdefaultdict(int)
    for word_str in pat.findall(text_chunk):
        word_bytes = word_str.encode("utf-8")
        word_chars = tuple(byte_to_unicode[b] for b in word_bytes)
        word_freqs[word_chars] += 1
    return word_freqs


def update_pair_stats(word: Tuple[str, ...], old_pair: Tuple[str, str], new_char: str, stats: Dict[Tuple[str, str], int], freq: int):
    i = 0
    n = len(word)
    while i < n:
        if i < n-1 and word[i] == old_pair[0] and word[i+1] == old_pair[1]:
            # ç§»é™¤æ—§pair
            if stats[old_pair] >= freq:
                stats[old_pair] -= freq
                if stats[old_pair] == 0:
                    del stats[old_pair]
            # æ›´æ–°å·¦ä¾§æ–°pair
            if i > 0:
                left_pair = (word[i-1], new_char)
                stats[left_pair] += freq
                old_left_pair = (word[i-1], word[i])
                if stats[old_left_pair] >= freq:
                    stats[old_left_pair] -= freq
                    if stats[old_left_pair] == 0:
                        del stats[old_left_pair]
            # æ›´æ–°å³ä¾§æ–°pair
            if i < n-2:
                right_pair = (new_char, word[i+2])
                stats[right_pair] += freq
                old_right_pair = (word[i+1], word[i+2])
                if stats[old_right_pair] >= freq:
                    stats[old_right_pair] -= freq
                    if stats[old_right_pair] == 0:
                        del stats[old_right_pair]
            i += 2
        else:
            i += 1
            

# --- Problem 3: BPE è®­ç»ƒå‡½æ•° ---



def train_bpe(data, vocab_size, special_tokens):
    # åˆå§‹åŒ–ï¼šæ–‡æœ¬è½¬å­—èŠ‚åºåˆ—ï¼ˆæ•´æ•°åˆ—è¡¨ï¼Œå¦‚"new" â†’ [110, 101, 119]ï¼‰
    tokens = list(data.encode('utf-8'))
    
    # åˆå§‹åŒ–vocabï¼šç‰¹æ®Štoken â†’ IDï¼ˆä¼˜å…ˆï¼‰ï¼Œå•å­—èŠ‚ â†’ ID
    vocab = {}
    # 1. æ·»åŠ ç‰¹æ®Štoken
    for idx, token in enumerate(special_tokens):
        vocab[token.encode('utf-8')] = idx  # ç‰¹æ®Štokenè½¬bytesä½œä¸ºé”®
    # 2. æ·»åŠ å•å­—èŠ‚tokenï¼ˆ0-255ï¼‰
    next_token_id = len(special_tokens)
    for byte in range(256):
        vocab[bytes([byte])] = next_token_id
        next_token_id += 1
    
    merges = []  # æœ€ç»ˆå­˜å‚¨(bytes, bytes)æ ¼å¼çš„åˆå¹¶è§„åˆ™
    pair_freq = defaultdict(int)
    
    # åˆå§‹åŒ–pairé¢‘ç‡
    for i in range(len(tokens)-1):
        pair = (tokens[i], tokens[i+1])
        pair_freq[pair] += 1

    while len(vocab) < vocab_size and pair_freq:
        # 1. æ‰¾é¢‘ç‡æœ€é«˜çš„pairï¼ˆæ•´æ•°å¯¹ï¼‰
        best_pair = max(pair_freq, key=pair_freq.get)  # å¦‚(101, 32)
        
        # 2. å°†best_pairè½¬ä¸ºbyteså¯¹ï¼ŒåŠ å…¥mergesï¼ˆåŒ¹é…æµ‹è¯•æ ¼å¼ï¼‰
        p1_bytes = bytes([best_pair[0]])
        p2_bytes = bytes([best_pair[1]])
        merges.append((p1_bytes, p2_bytes))  # ç›´æ¥å­˜byteså¯¹ï¼Œæ— éœ€åç»­è½¬æ¢
        
        # 3. æ–°å¢åˆå¹¶åçš„tokenåˆ°vocab
        merged_token = p1_bytes + p2_bytes
        new_id = next_token_id  # æ–°ID
        vocab[merged_token] = new_id
        next_token_id += 1
        
        # 4. åˆå¹¶tokenåºåˆ—ï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰
        new_tokens = []
        i = 0
        while i < len(tokens):
            if i < len(tokens)-1 and (tokens[i], tokens[i+1]) == best_pair:
                # ç”¨åˆå¹¶åçš„tokençš„IDï¼Ÿä¸ï¼Œè¿™é‡Œç»§ç»­ç”¨æ•´æ•°è¡¨ç¤ºï¼ˆæˆ–ç”¨æ–°IDï¼Œçœ‹é€»è¾‘ï¼‰
                # æ³¨æ„ï¼šå¦‚æœåç»­ç»Ÿè®¡éœ€è¦ï¼Œè¿™é‡Œå¯ä»¥ç”¨æ–°IDï¼Œä½†æ›´ç®€å•çš„æ˜¯æš‚æ—¶ç”¨tupleæ ‡è®°ï¼Œæœ€åç»Ÿä¸€è½¬bytes
                new_tokens.append(new_id)  # æˆ–ç”¨next_token_id-1ï¼ˆåˆšåˆ†é…çš„IDï¼‰
                i += 2
            else:
                new_tokens.append(tokens[i])
                i += 1
        tokens = new_tokens
        
        # 5. æ›´æ–°pairé¢‘ç‡ï¼ˆä¼˜åŒ–ï¼šæ¸…ç©ºé‡ç»Ÿè®¡ï¼Œå°æ•°æ®é‡è¶³å¤Ÿå¿«ï¼‰
        pair_freq.clear()
        for i in range(len(tokens)-1):
            # å¤„ç†åˆå¹¶åçš„tokenï¼ˆå¦‚æœæ˜¯tupleï¼Œè½¬ä¸ºç»Ÿä¸€çš„keyï¼‰
            left = tokens[i] if isinstance(tokens[i], int) else tuple(tokens[i])
            right = tokens[i+1] if isinstance(tokens[i+1], int) else tuple(tokens[i+1])
            pair_freq[(left, right)] += 1

    # æ³¨æ„ï¼šå¦‚æœæµ‹è¯•éœ€è¦vocabæ˜¯IDâ†’tokenï¼Œè¿™é‡Œåè½¬
    # vocab_id_to_token = {v: k for k, v in vocab.items()}
    return vocab, merges

   


# --- Problem 5: Tokenizer ç±»å®ç° (å·²ä¿®æ­£) ---

class BPE_Tokenizer:
    def __init__(self, vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], special_tokens: Optional[List[str]] = None):
        self.vocab = vocab
        # å°† merges è½¬ä¸ºå­—å…¸ï¼Œå€¼ä¸ºä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šä¼˜å…ˆï¼‰
        # é”®æ˜¯ (bytes, bytes)
        self.merges = {tuple(pair): i for i, pair in enumerate(merges)} 
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        PAT_STR = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
        self.pat = regex.compile(PAT_STR)

        # æ„å»ºç¼–ç å™¨ï¼šbytes -> ID
        self.encoder = {v: k for k, v in vocab.items()}
        # æ„å»ºè§£ç å™¨ï¼šID -> bytes
        self.decoder = vocab

        # å¤„ç†ç‰¹æ®Štoken
        self.special_tokens = set(special_tokens) if special_tokens else set()
        self.special_pattern = None
        self.special_encoder = {}
        if self.special_tokens:
            sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)
            pattern_str = "|".join(map(re.escape,sorted_special_tokens))
            self.special_pattern = regex.compile(f"({pattern_str})")
            # å»ºç«‹ç‰¹æ®Štokençš„å­—ç¬¦ä¸²åˆ°IDçš„æ˜ å°„
            for token_str in self.special_tokens:
                token_bytes = token_str.encode("utf-8")
                if token_bytes in self.encoder:
                    self.special_encoder[token_str] = self.encoder[token_bytes]

        # ç¼“å­˜
        self.cache = {}

    @classmethod
    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: Optional[List[str]] = None):
        with open(vocab_filepath, 'r', encoding='utf-8') as f:
            vocab_json = json.load(f)
            # å…³é”®ï¼šåŠ è½½ JSON æ—¶ï¼ŒKey æ˜¯å­—ç¬¦ä¸²ï¼Œå¿…é¡»è½¬ intï¼›Value æ˜¯å­—ç¬¦ä¸²ï¼Œå¿…é¡»è½¬ bytes
            # æ³¨æ„ï¼šè¿™é‡Œçš„ decode('unicode_escape').encode('latin1') æ˜¯ä¸ºäº†è¿˜åŸè¢« json åºåˆ—åŒ–æ—¶çš„å­—èŠ‚
            vocab = {}
            for k, v in vocab_json.items():
                vocab[int(k)] = v.encode('utf-8').decode('unicode_escape').encode('latin1')

        merges = []
        with open(merges_filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line: continue
                # mergesæ–‡ä»¶é€šå¸¸æ˜¯ "tokenA tokenB"
                parts = line.split()
                if len(parts) != 2: continue # è·³è¿‡ç©ºè¡Œæˆ–æ ¼å¼é”™è¯¯çš„è¡Œ
                p1, p2 = parts
                p1_bytes = p1.encode('utf-8').decode('unicode_escape').encode('latin1')
                p2_bytes = p2.encode('utf-8').decode('unicode_escape').encode('latin1')
                merges.append((p1_bytes, p2_bytes))
        
        return cls(vocab, merges, special_tokens)

    def save(self, vocab_filepath: str, merges_filepath: str):
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_json_save = {k: v.decode('latin1').encode('unicode_escape').decode('utf-8') for k, v in self.vocab.items()}
        with open(vocab_filepath, 'w', encoding='utf-8') as f:
            json.dump(vocab_json_save, f, ensure_ascii=False, indent=2)

        # ä¿å­˜åˆå¹¶è§„åˆ™
        with open(merges_filepath, 'w', encoding='utf-8') as f:
            for p1, p2 in self.merges.keys():
                 p1_str = p1.decode('latin1').encode('unicode_escape').decode('utf-8')
                 p2_str = p2.decode('latin1').encode('unicode_escape').decode('utf-8')
                 f.write(f"{p1_str} {p2_str}\n")
    
    def _bpe_merge(self, word_bytes: bytes) -> List[int]:
        """
        å¯¹å•ä¸ªå•è¯è¿›è¡ŒBPEåˆå¹¶ã€‚
        æ³¨æ„ï¼šè¿™é‡Œæ“ä½œçš„æ˜¯ Token IDï¼Œè€Œä¸æ˜¯åŸå§‹å­—èŠ‚å€¼ã€‚
        """
        if word_bytes in self.cache:
            return self.cache[word_bytes]

        # 1. åˆå§‹æ­¥éª¤ï¼šå°†å­—èŠ‚åºåˆ—è½¬æ¢ä¸º ID åºåˆ—
        # ä¿®æ­£ä»£ç ï¼šæŸ¥ encoder è¡¨
        tokens = [self.encoder[bytes([b])] for b in word_bytes]

        while len(tokens) >= 2:
            # å¯»æ‰¾å½“å‰ tokens åˆ—è¡¨ä¸­æ‰€æœ‰ç›¸é‚»å¯¹ä¸­ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼ˆrankå€¼æœ€å°ï¼‰çš„ä¸€å¯¹
            stats = {}
            for i in range(len(tokens) - 1):
                # è·å–ç›¸é‚»ä¸¤ä¸ª ID å¯¹åº”çš„å­—èŠ‚åºåˆ—
                p1_bytes = self.decoder[tokens[i]]
                p2_bytes = self.decoder[tokens[i+1]]
                pair = (p1_bytes, p2_bytes)
                
                # å¦‚æœè¿™ä¸ªå¯¹åœ¨åˆå¹¶è§„åˆ™é‡Œï¼Œè®°å½•å®ƒçš„ä¼˜å…ˆçº§
                if pair in self.merges:
                    stats[pair] = self.merges[pair]

            # å¦‚æœæ²¡æœ‰å¯åˆå¹¶çš„å¯¹ï¼Œé€€å‡ºå¾ªç¯
            if not stats:
                break

            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜ï¼ˆæ•°å€¼æœ€å°ï¼‰çš„å¯¹
            best_pair = min(stats, key=stats.get)
            
            # è®¡ç®—åˆå¹¶åçš„æ–° Token çš„ ID
            # æ³¨æ„ï¼šåˆå¹¶åçš„ bytes = p1_bytes + p2_bytes
            merged_bytes = best_pair[0] + best_pair[1]
            new_id = self.encoder[merged_bytes]

            # æ‰§è¡Œåˆå¹¶ï¼šåœ¨ tokens åˆ—è¡¨ä¸­æ›¿æ¢æ‰æ‰€æœ‰çš„ best_pair
            new_tokens = []
            i = 0
            while i < len(tokens):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬è¦åˆå¹¶çš„å¯¹
                # éœ€è¦å†æ¬¡æŸ¥è¡¨ç¡®è®¤ current bytes æ˜¯å¦åŒ¹é…
                if i < len(tokens) - 1:
                    b1 = self.decoder[tokens[i]]
                    b2 = self.decoder[tokens[i+1]]
                    if (b1, b2) == best_pair:
                        new_tokens.append(new_id)
                        i += 2
                        continue
                
                new_tokens.append(tokens[i])
                i += 1
            
            tokens = new_tokens
        
        self.cache[word_bytes] = tokens
        return tokens

    def encode(self, text: str) -> List[int]:
        """å°†å­—ç¬¦ä¸²ç¼–ç ä¸º token ID åˆ—è¡¨"""
        token_ids = []
        
        # å¤„ç†ç‰¹æ®Š token
        if self.special_pattern:
            chunks = self.special_pattern.split(text)
            for i, chunk in enumerate(chunks):
                if i % 2 == 1: # ç‰¹æ®Š token
                    if chunk in self.special_encoder:
                        token_ids.append(self.special_encoder[chunk])
                    else:
                        print(f"Warning: Special token {chunk} not found in vocab.")
                else: # æ™®é€šæ–‡æœ¬
                    if chunk:
                        for word in self.pat.findall(chunk):
                            word_bytes = word.encode("utf-8")
                            token_ids.extend(self._bpe_merge(word_bytes))
        else:
            for word in self.pat.findall(text):
                word_bytes = word.encode("utf-8")
                token_ids.extend(self._bpe_merge(word_bytes))

        return token_ids

    # --- æ–°å¢çš„æ–¹æ³•ï¼šProblem 6 è¦æ±‚ ---
    def encode_iterable(self, text_iterable: Iterable[str]) -> Iterator[int]:
        """
        å¯¹ä¸€ä¸ªæ–‡æœ¬è¿­ä»£å™¨è¿›è¡Œç¼–ç ã€‚
        è¿™ç”¨äºå¤„ç†å¤§å‹æ•°æ®é›†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ–‡æœ¬ã€‚
        """
        for text in text_iterable:
            yield from self.encode(text) #è¿”å›æ•´æ•°ID

    def decode(self, ids: List[int]) -> str:
        """å°† token ID åˆ—è¡¨è§£ç ä¸ºå­—ç¬¦ä¸²"""
        # æ³¨æ„ï¼šä½¿ç”¨ self.decoder æŠŠ ID è½¬å› bytes
        # errors='replace' é˜²æ­¢éæ³•çš„ UTF-8 åºåˆ—å¯¼è‡´å´©æºƒ
        all_bytes = b"".join(self.decoder[i] for i in ids)
        text = all_bytes.decode("utf-8", errors='replace')
        return text

        
# --- ä¸»æ‰§è¡Œå— (ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º) ---
if __name__ == '__main__':
    # ä¿®å¤ 3: æ¢å¤äº†æ•°æ®ç”Ÿæˆä»£ç ï¼Œé¿å… FileNotFoundError
    import time
    import resource
    import os

    # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
    INPUT_PATH = "train_dummy.txt" 
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ‘ä»¬å°±ç°åœºé€ ä¸€ä¸ªï¼
    if not os.path.exists(INPUT_PATH):
        print(f"æ­£åœ¨ç”Ÿæˆæµ‹è¯•æ•°æ®åˆ° {INPUT_PATH} ...")
        with open(INPUT_PATH, "w", encoding="utf-8") as f:
            data = f.read()  # è¯»å–æ–‡ä»¶å†…å®¹
        vocab, merges = train_bpe(data, VOCAB_SIZE, SPECIAL_TOKENS)  # ä¼ å…¥å†…å®¹è€Œéè·¯å¾„
            f.write("low low low low low\n")
            f.write("lower lower widest widest widest\n")
            f.write("newest newest newest newest newest newest\n")
            f.write("This is a simple test. Emoji: ğŸ˜Š. Chinese: è¿™é‡Œæœ‰ä¸€äº›ä¸­æ–‡æµ‹è¯•æ•°æ®ã€‚\n")
            f.write("The quick brown fox jumps over the lazy dog. " * 50)

    # è®­ç»ƒå‚æ•°
    VOCAB_SIZE = 500
    SPECIAL_TOKENS = ["<|endoftext|>"]
  

    # (a) è®­ç»ƒåˆ†è¯å™¨
    print("å¼€å§‹è®­ç»ƒBPEåˆ†è¯å™¨...")
    start_time = time.time()
    
    vocab, merges = train_bpe(INPUT_PATH, VOCAB_SIZE, SPECIAL_TOKENS)
    
    end_time = time.time()
    training_time = end_time - start_time
    
    # æŠ¥å‘Šè®­ç»ƒæ—¶é—´å’Œå†…å­˜å ç”¨
    memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024  # in MB
    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    print(f"å³°å€¼å†…å­˜å ç”¨: {memory_usage:.2f} MB")

    # è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token
    longest_token = max(vocab.values(), key=len)
    print(f"è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token (bytes): {longest_token}")
    print(f"è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token (str): '{longest_token.decode('utf-8', 'replace')}'")

    # ä¿å­˜è®­ç»ƒç»“æœ
    VOCAB_FILE = "tinystories_vocab.json"
    MERGES_FILE = "tinystories_merges.txt"
    tokenizer_for_saving = BPE_Tokenizer(vocab, merges, SPECIAL_TOKENS)
    tokenizer_for_saving.save(VOCAB_FILE, MERGES_FILE)
    print(f"è¯æ±‡è¡¨å·²ä¿å­˜åˆ° {VOCAB_FILE}")
    print(f"åˆå¹¶è§„åˆ™å·²ä¿å­˜åˆ° {MERGES_FILE}")
    
    # --- Problem 5 & 6: ä½¿ç”¨Tokenizerç±» ---
    print("\n--- Tokenizer å®éªŒ ---")
    
    # å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä»å†…å­˜åŠ è½½ï¼Œé¿å…ä¿å­˜/è¯»å–æ—¶çš„ç¼–ç é—®é¢˜
    tokenizer = BPE_Tokenizer(vocab, merges, SPECIAL_TOKENS)
    
    # æµ‹è¯•ç¼–ç å’Œè§£ç 
    text_to_test = "newest low lower ğŸ˜Šä½ å¥½<|endoftext|>"
    encoded = tokenizer.encode(text_to_test)
    decoded = tokenizer.decode(encoded)
    
    print(f"åŸå§‹æ–‡æœ¬: '{text_to_test}'")
    print(f"ç¼–ç ç»“æœ (token IDs): {encoded}")
    print(f"è§£ç ç»“æœ: '{decoded}'")
    
    if text_to_test == decoded:
        print("âœ… ç¼–ç  -> è§£ç  ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ è­¦å‘Šï¼šè§£ç ä¸åŒ¹é…")

    # (a) è®¡ç®—å‹ç¼©ç‡
    sample_text = "This is a sample document from TinyStories dataset to calculate the compression ratio."
    encoded_sample = tokenizer.encode(sample_text)
    num_bytes = len(sample_text.encode("utf-8"))
    num_tokens = len(encoded_sample)
    compression_ratio = num_bytes / num_tokens
    print(f"\n(a) å‹ç¼©ç‡ (bytes/token): {compression_ratio:.2f} ({num_bytes} bytes / {num_tokens} tokens)")

    # (b) ä¼°ç®—ååé‡
    large_text = sample_text * 1000
    start_time_enc = time.time()
    tokenizer.encode(large_text)
    end_time_enc = time.time()
    duration_enc = end_time_enc - start_time_enc
    if duration_enc > 0:
        throughput = len(large_text.encode("utf-8")) / duration_enc / 1e6 # MB/s
        print(f"(b) ç¼–ç ååé‡: {throughput:.2f} MB/s")
