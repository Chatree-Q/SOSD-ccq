import json
import os
import re
import regex # ä½¿ç”¨ç¬¬ä¸‰æ–¹regexåº“ä»¥æ›´å¥½åœ°æ”¯æŒ\p{L}ç­‰Unicodeå±æ€§
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
from functools import partial
from typing import Dict, List, Tuple, Optional, Iterable, Iterator

# --- æ ¸å¿ƒè¾…åŠ©å‡½æ•° ---

def bytes_to_unicode():
    """GPT-2çš„å­—èŠ‚åˆ°Unicodeå­—ç¬¦æ˜ å°„ï¼Œé¿å…æ§åˆ¶å­—ç¬¦é—®é¢˜"""
    bs = list(range(ord("!"), ord("~")+1)) + list(range(ord("Â¡"), ord("Â¬")+1)) + list(range(ord("Â®"), ord("Ã¿")+1))
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8 + n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))

# å…¨å±€æ˜ å°„è¡¨
byte_to_unicode = bytes_to_unicode()
unicode_to_byte = {v: k for k, v in byte_to_unicode.items()}

#def get_pair_stats_optimized(word_freqs: Dict[Tuple[int, ...], int]) -> Dict[Tuple[int, int], int]:
#    """
#    ä»è¯é¢‘å­—å…¸ä¸­é«˜æ•ˆåœ°è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡ã€‚
#    è¿™æ˜¯ä¼˜åŒ–çš„å…³é”®ï¼šæˆ‘ä»¬ä¸éå†æ•´ä¸ªæ–‡æœ¬ï¼Œè€Œæ˜¯éå†è¯æ±‡è¡¨å¹¶ä¹˜ä»¥å…¶é¢‘ç‡ã€‚
#    """
#    stats = {}
#    for word, freq in word_freqs.items():
#        for i in range(len(word) - 1):
#            pair = (word[i], word[i+1])
#            stats[pair] = stats.get(pair, 0) + freq
#    return stats

def merge_word_freqs_optimized(word_freqs: Dict[Tuple[str, ...], int], pair: Tuple[str, str], new_char: str) -> Dict[Tuple[str, ...], int]:
    new_word_freqs = {}
    for word, freq in word_freqs.items():
        new_word = []
        i = 0
        while i < len(word):
            if i < len(word)-1 and (word[i], word[i+1]) == pair:
                new_word.append(new_char)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        new_word_freqs[tuple(new_word)] = new_word_freqs.get(tuple(new_word), 0) + freq
    return new_word_freqs


def pretokenize_chunk(text_chunk: str, pat_str: str) -> Dict[Tuple[str, ...], int]:
    """å¹¶è¡ŒåŒ–é¢„åˆ†è¯çš„å·¥ä½œå‡½æ•°ï¼ˆè¿”å›Unicodeå­—ç¬¦tupleçš„è¯é¢‘ï¼‰"""
    pat = regex.compile(pat_str)
    word_freqs = {}
    for word_str in pat.findall(text_chunk):
        # å°†word_strè½¬ä¸ºå­—èŠ‚ï¼Œå†æ˜ å°„ä¸ºUnicodeå­—ç¬¦åºåˆ—
        word_bytes = word_str.encode("utf-8")
        word_chars = tuple(byte_to_unicode[b] for b in word_bytes)
        word_freqs[word_chars] = word_freqs.get(word_chars, 0) + 1
    return word_freqs


# --- Problem 3: BPE è®­ç»ƒå‡½æ•° ---

# ä¿®å¤ 1: æŠŠå‡½æ•°å®šä¹‰æ”¾åœ¨ä¸€è¡Œï¼Œè§£å†³äº† SyntaxError
def train_bpe(input_path: str, vocab_size: int, special_tokens: List[str]) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:
    """
    è®­ç»ƒä¸€ä¸ªå­—èŠ‚çº§çš„BPEåˆ†è¯å™¨ã€‚

    Args:
        input_path: è®­ç»ƒæ•°æ®è·¯å¾„ã€‚
        vocab_size: ç›®æ ‡è¯æ±‡è¡¨å¤§å°ã€‚
        special_tokens: ç‰¹æ®Štokenåˆ—è¡¨ã€‚

    Returns:
        A tuple containing:
            - vocab: ä»token IDåˆ°å…¶å­—èŠ‚åºåˆ—çš„æ˜ å°„ã€‚
            - merges: æŒ‰åˆ›å»ºé¡ºåºåˆ—å‡ºçš„BPEåˆå¹¶è§„åˆ™ã€‚
    """

    # 1. è¯æ±‡è¡¨åˆå§‹åŒ– (Vocabulary initialization)
    vocab = {i: bytes([i]) for i in range(256)}
    next_token_id = 256
    for i, token_str in enumerate(special_tokens):
        # å°†ç‰¹æ®Štokenæ”¾åœ¨è¯æ±‡è¡¨çš„æœ«å°¾ï¼ŒIDä»vocab_size-1å¼€å§‹é€’å‡
        # è¿™æ ·å¯ä»¥ç¡®ä¿å®ƒä»¬ä¸ä¼šä¸åˆå¹¶çš„token IDå†²çª
        vocab[next_token_id] = token_str.encode("utf-8")
        next_token_id += 1


    # GPT-2çš„æ­£åˆ™è¡¨è¾¾å¼
    PAT_STR = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    
    # è¯»å–è¯­æ–™åº“
    with open(input_path, 'r', encoding='utf-8') as f:
        text = f.read()

    # 2. é¢„åˆ†è¯ (Pre-tokenization)
    # é¦–å…ˆæŒ‰ç‰¹æ®Štokenåˆ†å‰²è¯­æ–™åº“
    special_pattern = "|".join(map(re.escape, special_tokens))
    text_chunks = re.split(f"({special_pattern})", text)

    # è¿‡æ»¤å‡ºéç‰¹æ®Štokençš„æ–‡æœ¬å—
    work_chunks = [chunk for i, chunk in enumerate(text_chunks) if i % 2 == 0 and chunk]
    
    # ä¿®å¤ 2: ä¿®æ­£äº† if/else çš„ç¼©è¿›é—®é¢˜
    # å¦‚æœæ•°æ®é‡å¾ˆå°ï¼ˆæ¯”å¦‚æµ‹è¯•ç”¨çš„ corpus.en åªæœ‰å‡ KBï¼‰ï¼Œå¼ºåˆ¶å•è¿›ç¨‹
    if len(text) < 5_000_000: # 5MB ä»¥ä¸‹å•è¿›ç¨‹
        chunk_freqs_list = [pretokenize_chunk(chunk, PAT_STR) for chunk in work_chunks]
    else:
        # å¹¶è¡ŒåŒ–é¢„åˆ†è¯
        num_procs = min(cpu_count(), os.cpu_count() or 1)
        with Pool(num_procs) as pool:
            worker = partial(pretokenize_chunk, pat_str=PAT_STR)
            chunk_freqs_list = list(tqdm(
                pool.imap(worker, work_chunks),
                total=len(work_chunks),
                desc="å¹¶è¡Œé¢„åˆ†è¯"
            ))
    
    # åˆå¹¶æ‰€æœ‰è¿›ç¨‹çš„ç»“æœ
    word_freqs = {}
    for chunk_freqs in chunk_freqs_list:
        for word, freq in chunk_freqs.items():
            word_freqs[word] = word_freqs.get(word, 0) + freq
            
    def init_pair_stats(word_freqs: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, str], int]:
        stats = {}
        for word, freq in word_freqs.items():
            for i in range(len(word)-1):
                pair = (word[i], word[i+1])
                stats[pair] = stats.get(pair, 0) + freq
        return stats
            
    stats = init_pair_stats(word_freqs)  # åªæ‰§è¡Œä¸€æ¬¡ï¼Œåˆå§‹åŒ–æ‰€æœ‰pairçš„é¢‘ç‡
    


    # 3. è®¡ç®— BPE åˆå¹¶ (Compute BPE merges)
    num_merges = vocab_size - len(vocab) 
    merges_list = []  # ä½¿ç”¨åˆ—è¡¨æ¥ä¿è¯é¡ºåº
    
    bpe_merges = {}  # è®°å½•åˆå¹¶è§„åˆ™ï¼š(char1, char2) -> new_char

    
    pbar = tqdm(range(num_merges), desc="BPE åˆå¹¶")
    for i in pbar:
        # (a) ç»Ÿè®¡æ‰€æœ‰ç›¸é‚» token å¯¹çš„é¢‘ç‡
        if not stats:
            print("æ²¡æœ‰æ›´å¤šçš„å¯¹å¯ä»¥åˆå¹¶ï¼Œæå‰åœæ­¢ã€‚")
            break

        # (b) æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„ token å¯¹ï¼Œå¹¶å¤„ç†å¹³å±€
        # ä¿®æ­£æ’åºè§„åˆ™ï¼šé¢‘ç‡é«˜ä¼˜å…ˆï¼Œé¢‘ç‡ç›¸åŒæŒ‰Unicodeå­—ç¬¦é¡ºåº
        best_pair = max(stats.items(), key=lambda x: (x[1], x[0]))[0]
        
         # ç”Ÿæˆæ–°çš„åˆå¹¶å­—ç¬¦ï¼ˆå¦‚'Ä t'ï¼‰
        new_char = best_pair[0] + best_pair[1]
        bpe_merges[best_pair] = new_char

         #ï¼ˆcï¼‰ å¢é‡æ›´æ–°word_freqså’Œstats
        word_freqs = merge_word_freqs_optimized(word_freqs, best_pair, new_char)
         # ========== æ–°å¢ï¼šå¢é‡æ›´æ–°statsï¼ˆåªæ›´æ–°å—å½±å“çš„pairï¼‰ ==========
        def update_pair_stats(word: Tuple[str, ...], old_pair: Tuple[str, str], new_char: str, 
                      stats: Dict[Tuple[str, str], int], freq: int):
            i = 0
            while i < len(word)-1:
                if (word[i], word[i+1]) == old_pair:
                    # ç§»é™¤æ—§pairçš„é¢‘ç‡
                    if old_pair in stats:
                        stats[old_pair] -= freq
                        if stats[old_pair] <= 0:
                            del stats[old_pair]
                    # æ›´æ–°å·¦ä¾§ç›¸é‚»å¯¹
                    if i > 0:
                        left_pair = (word[i-1], new_char)
                        stats[left_pair] = stats.get(left_pair, 0) + freq
                        old_left = (word[i-1], word[i])
                        stats[old_left] -= freq
                        if stats[old_left] <= 0:
                            del stats[old_left]
                    # æ›´æ–°å³ä¾§ç›¸é‚»å¯¹
                    if i+2 < len(word):
                        right_pair = (new_char, word[i+2])
                        stats[right_pair] = stats.get(right_pair, 0) + freq
                        old_right = (word[i+1], word[i+2])
                        stats[old_right] -= freq
                        if stats[old_right] <= 0:
                            del stats[old_right]
                    i += 2
                else:
                    i += 1
        for word, freq in word_freqs.items():
            update_pair_stats(word, best_pair, new_char, stats, freq)


        # (d) å°† "AB" æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­
        
        p1_bytes = bytes([unicode_to_byte[c] for c in best_pair[0]])
        p2_bytes = bytes([unicode_to_byte[c] for c in best_pair[1]])
        merges_list.append((p1_bytes, p2_bytes))


        # (e) å°† ("A", "B") è®°å½•åˆ°åˆå¹¶è§„åˆ™åˆ—è¡¨ merges ä¸­
         # æ›´æ–°vocabï¼ˆnew_charè½¬å›bytesï¼‰
        new_char_bytes = bytes([unicode_to_byte[c] for c in new_char])
        vocab[next_token_id] = new_char_bytes
        
        next_token_id += 1

    return vocab, merges_list
   


# --- Problem 5: Tokenizer ç±»å®ç° (å·²ä¿®æ­£) ---

class BPE_Tokenizer:
    def __init__(self, vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], special_tokens: Optional[List[str]] = None):
        self.vocab = vocab
        # å°† merges è½¬ä¸ºå­—å…¸ï¼Œå€¼ä¸ºä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šä¼˜å…ˆï¼‰
        # é”®æ˜¯ (bytes, bytes)
        self.merges = {tuple(pair): i for i, pair in enumerate(merges)} 
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        PAT_STR = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
        self.pat = regex.compile(PAT_STR)

        # æ„å»ºç¼–ç å™¨ï¼šbytes -> ID
        self.encoder = {v: k for k, v in vocab.items()}
        # æ„å»ºè§£ç å™¨ï¼šID -> bytes
        self.decoder = vocab

        # å¤„ç†ç‰¹æ®Štoken
        self.special_tokens = set(special_tokens) if special_tokens else set()
        self.special_pattern = None
        self.special_encoder = {}
        if self.special_tokens:
            sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)
            pattern_str = "|".join(map(re.escape,sorted_special_tokens))
            self.special_pattern = regex.compile(f"({pattern_str})")
            # å»ºç«‹ç‰¹æ®Štokençš„å­—ç¬¦ä¸²åˆ°IDçš„æ˜ å°„
            for token_str in self.special_tokens:
                token_bytes = token_str.encode("utf-8")
                if token_bytes in self.encoder:
                    self.special_encoder[token_str] = self.encoder[token_bytes]

        # ç¼“å­˜
        self.cache = {}

    @classmethod
    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: Optional[List[str]] = None):
        with open(vocab_filepath, 'r', encoding='utf-8') as f:
            vocab_json = json.load(f)
            # å…³é”®ï¼šåŠ è½½ JSON æ—¶ï¼ŒKey æ˜¯å­—ç¬¦ä¸²ï¼Œå¿…é¡»è½¬ intï¼›Value æ˜¯å­—ç¬¦ä¸²ï¼Œå¿…é¡»è½¬ bytes
            # æ³¨æ„ï¼šè¿™é‡Œçš„ decode('unicode_escape').encode('latin1') æ˜¯ä¸ºäº†è¿˜åŸè¢« json åºåˆ—åŒ–æ—¶çš„å­—èŠ‚
            vocab = {}
            for k, v in vocab_json.items():
                vocab[int(k)] = v.encode('utf-8').decode('unicode_escape').encode('latin1')

        merges = []
        with open(merges_filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line: continue
                # mergesæ–‡ä»¶é€šå¸¸æ˜¯ "tokenA tokenB"
                parts = line.split()
                if len(parts) != 2: continue # è·³è¿‡ç©ºè¡Œæˆ–æ ¼å¼é”™è¯¯çš„è¡Œ
                p1, p2 = parts
                p1_bytes = p1.encode('utf-8').decode('unicode_escape').encode('latin1')
                p2_bytes = p2.encode('utf-8').decode('unicode_escape').encode('latin1')
                merges.append((p1_bytes, p2_bytes))
        
        return cls(vocab, merges, special_tokens)

    def save(self, vocab_filepath: str, merges_filepath: str):
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_json_save = {k: v.decode('latin1').encode('unicode_escape').decode('utf-8') for k, v in self.vocab.items()}
        with open(vocab_filepath, 'w', encoding='utf-8') as f:
            json.dump(vocab_json_save, f, ensure_ascii=False, indent=2)

        # ä¿å­˜åˆå¹¶è§„åˆ™
        with open(merges_filepath, 'w', encoding='utf-8') as f:
            for p1, p2 in self.merges.keys():
                 p1_str = p1.decode('latin1').encode('unicode_escape').decode('utf-8')
                 p2_str = p2.decode('latin1').encode('unicode_escape').decode('utf-8')
                 f.write(f"{p1_str} {p2_str}\n")
    
    def _bpe_merge(self, word_bytes: bytes) -> List[int]:
        """
        å¯¹å•ä¸ªå•è¯è¿›è¡ŒBPEåˆå¹¶ã€‚
        æ³¨æ„ï¼šè¿™é‡Œæ“ä½œçš„æ˜¯ Token IDï¼Œè€Œä¸æ˜¯åŸå§‹å­—èŠ‚å€¼ã€‚
        """
        if word_bytes in self.cache:
            return self.cache[word_bytes]

        # 1. åˆå§‹æ­¥éª¤ï¼šå°†å­—èŠ‚åºåˆ—è½¬æ¢ä¸º ID åºåˆ—
        # ä¿®æ­£ä»£ç ï¼šæŸ¥ encoder è¡¨
        tokens = [self.encoder[bytes([b])] for b in word_bytes]

        while len(tokens) >= 2:
            # å¯»æ‰¾å½“å‰ tokens åˆ—è¡¨ä¸­æ‰€æœ‰ç›¸é‚»å¯¹ä¸­ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼ˆrankå€¼æœ€å°ï¼‰çš„ä¸€å¯¹
            stats = {}
            for i in range(len(tokens) - 1):
                # è·å–ç›¸é‚»ä¸¤ä¸ª ID å¯¹åº”çš„å­—èŠ‚åºåˆ—
                p1_bytes = self.decoder[tokens[i]]
                p2_bytes = self.decoder[tokens[i+1]]
                pair = (p1_bytes, p2_bytes)
                
                # å¦‚æœè¿™ä¸ªå¯¹åœ¨åˆå¹¶è§„åˆ™é‡Œï¼Œè®°å½•å®ƒçš„ä¼˜å…ˆçº§
                if pair in self.merges:
                    stats[pair] = self.merges[pair]

            # å¦‚æœæ²¡æœ‰å¯åˆå¹¶çš„å¯¹ï¼Œé€€å‡ºå¾ªç¯
            if not stats:
                break

            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜ï¼ˆæ•°å€¼æœ€å°ï¼‰çš„å¯¹
            best_pair = min(stats, key=stats.get)
            
            # è®¡ç®—åˆå¹¶åçš„æ–° Token çš„ ID
            # æ³¨æ„ï¼šåˆå¹¶åçš„ bytes = p1_bytes + p2_bytes
            merged_bytes = best_pair[0] + best_pair[1]
            new_id = self.encoder[merged_bytes]

            # æ‰§è¡Œåˆå¹¶ï¼šåœ¨ tokens åˆ—è¡¨ä¸­æ›¿æ¢æ‰æ‰€æœ‰çš„ best_pair
            new_tokens = []
            i = 0
            while i < len(tokens):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬è¦åˆå¹¶çš„å¯¹
                # éœ€è¦å†æ¬¡æŸ¥è¡¨ç¡®è®¤ current bytes æ˜¯å¦åŒ¹é…
                if i < len(tokens) - 1:
                    b1 = self.decoder[tokens[i]]
                    b2 = self.decoder[tokens[i+1]]
                    if (b1, b2) == best_pair:
                        new_tokens.append(new_id)
                        i += 2
                        continue
                
                new_tokens.append(tokens[i])
                i += 1
            
            tokens = new_tokens
        
        self.cache[word_bytes] = tokens
        return tokens

    def encode(self, text: str) -> List[int]:
        """å°†å­—ç¬¦ä¸²ç¼–ç ä¸º token ID åˆ—è¡¨"""
        token_ids = []
        
        # å¤„ç†ç‰¹æ®Š token
        if self.special_pattern:
            chunks = self.special_pattern.split(text)
            for i, chunk in enumerate(chunks):
                if i % 2 == 1: # ç‰¹æ®Š token
                    if chunk in self.special_encoder:
                        token_ids.append(self.special_encoder[chunk])
                    else:
                        print(f"Warning: Special token {chunk} not found in vocab.")
                else: # æ™®é€šæ–‡æœ¬
                    if chunk:
                        for word in self.pat.findall(chunk):
                            word_bytes = word.encode("utf-8")
                            token_ids.extend(self._bpe_merge(word_bytes))
        else:
            for word in self.pat.findall(text):
                word_bytes = word.encode("utf-8")
                token_ids.extend(self._bpe_merge(word_bytes))

        return token_ids

    # --- æ–°å¢çš„æ–¹æ³•ï¼šProblem 6 è¦æ±‚ ---
    def encode_iterable(self, text_iterable: Iterable[str]) -> Iterator[int]:
        """
        å¯¹ä¸€ä¸ªæ–‡æœ¬è¿­ä»£å™¨è¿›è¡Œç¼–ç ã€‚
        è¿™ç”¨äºå¤„ç†å¤§å‹æ•°æ®é›†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ–‡æœ¬ã€‚
        """
        for text in text_iterable:
            yield from self.encode(text) #è¿”å›æ•´æ•°ID

    def decode(self, ids: List[int]) -> str:
        """å°† token ID åˆ—è¡¨è§£ç ä¸ºå­—ç¬¦ä¸²"""
        # æ³¨æ„ï¼šä½¿ç”¨ self.decoder æŠŠ ID è½¬å› bytes
        # errors='replace' é˜²æ­¢éæ³•çš„ UTF-8 åºåˆ—å¯¼è‡´å´©æºƒ
        all_bytes = b"".join(self.decoder[i] for i in ids)
        text = all_bytes.decode("utf-8", errors='replace')
        return text

        
# --- ä¸»æ‰§è¡Œå— (ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º) ---
if __name__ == '__main__':
    # ä¿®å¤ 3: æ¢å¤äº†æ•°æ®ç”Ÿæˆä»£ç ï¼Œé¿å… FileNotFoundError
    import time
    import resource
    import os

    # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
    INPUT_PATH = "train_dummy.txt" 
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ‘ä»¬å°±ç°åœºé€ ä¸€ä¸ªï¼
    if not os.path.exists(INPUT_PATH):
        print(f"æ­£åœ¨ç”Ÿæˆæµ‹è¯•æ•°æ®åˆ° {INPUT_PATH} ...")
        with open(INPUT_PATH, "w", encoding="utf-8") as f:
            f.write("low low low low low\n")
            f.write("lower lower widest widest widest\n")
            f.write("newest newest newest newest newest newest\n")
            f.write("This is a simple test. Emoji: ğŸ˜Š. Chinese: è¿™é‡Œæœ‰ä¸€äº›ä¸­æ–‡æµ‹è¯•æ•°æ®ã€‚\n")
            f.write("The quick brown fox jumps over the lazy dog. " * 50)

    # è®­ç»ƒå‚æ•°
    VOCAB_SIZE = 500
    SPECIAL_TOKENS = ["<|endoftext|>"]
  

    # (a) è®­ç»ƒåˆ†è¯å™¨
    print("å¼€å§‹è®­ç»ƒBPEåˆ†è¯å™¨...")
    start_time = time.time()
    
    vocab, merges = train_bpe(INPUT_PATH, VOCAB_SIZE, SPECIAL_TOKENS)
    
    end_time = time.time()
    training_time = end_time - start_time
    
    # æŠ¥å‘Šè®­ç»ƒæ—¶é—´å’Œå†…å­˜å ç”¨
    memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024  # in MB
    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    print(f"å³°å€¼å†…å­˜å ç”¨: {memory_usage:.2f} MB")

    # è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token
    longest_token = max(vocab.values(), key=len)
    print(f"è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token (bytes): {longest_token}")
    print(f"è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token (str): '{longest_token.decode('utf-8', 'replace')}'")

    # ä¿å­˜è®­ç»ƒç»“æœ
    VOCAB_FILE = "tinystories_vocab.json"
    MERGES_FILE = "tinystories_merges.txt"
    tokenizer_for_saving = BPE_Tokenizer(vocab, merges, SPECIAL_TOKENS)
    tokenizer_for_saving.save(VOCAB_FILE, MERGES_FILE)
    print(f"è¯æ±‡è¡¨å·²ä¿å­˜åˆ° {VOCAB_FILE}")
    print(f"åˆå¹¶è§„åˆ™å·²ä¿å­˜åˆ° {MERGES_FILE}")
    
    # --- Problem 5 & 6: ä½¿ç”¨Tokenizerç±» ---
    print("\n--- Tokenizer å®éªŒ ---")
    
    # å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä»å†…å­˜åŠ è½½ï¼Œé¿å…ä¿å­˜/è¯»å–æ—¶çš„ç¼–ç é—®é¢˜
    tokenizer = BPE_Tokenizer(vocab, merges, SPECIAL_TOKENS)
    
    # æµ‹è¯•ç¼–ç å’Œè§£ç 
    text_to_test = "newest low lower ğŸ˜Šä½ å¥½<|endoftext|>"
    encoded = tokenizer.encode(text_to_test)
    decoded = tokenizer.decode(encoded)
    
    print(f"åŸå§‹æ–‡æœ¬: '{text_to_test}'")
    print(f"ç¼–ç ç»“æœ (token IDs): {encoded}")
    print(f"è§£ç ç»“æœ: '{decoded}'")
    
    if text_to_test == decoded:
        print("âœ… ç¼–ç  -> è§£ç  ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ è­¦å‘Šï¼šè§£ç ä¸åŒ¹é…")

    # (a) è®¡ç®—å‹ç¼©ç‡
    sample_text = "This is a sample document from TinyStories dataset to calculate the compression ratio."
    encoded_sample = tokenizer.encode(sample_text)
    num_bytes = len(sample_text.encode("utf-8"))
    num_tokens = len(encoded_sample)
    compression_ratio = num_bytes / num_tokens
    print(f"\n(a) å‹ç¼©ç‡ (bytes/token): {compression_ratio:.2f} ({num_bytes} bytes / {num_tokens} tokens)")

    # (b) ä¼°ç®—ååé‡
    large_text = sample_text * 1000
    start_time_enc = time.time()
    tokenizer.encode(large_text)
    end_time_enc = time.time()
    duration_enc = end_time_enc - start_time_enc
    if duration_enc > 0:
        throughput = len(large_text.encode("utf-8")) / duration_enc / 1e6 # MB/s
        print(f"(b) ç¼–ç ååé‡: {throughput:.2f} MB/s")
