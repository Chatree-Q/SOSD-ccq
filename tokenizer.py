import json
import os
import re
import regex # ä½¿ç”¨ç¬¬ä¸‰æ–¹regexåº“ä»¥æ›´å¥½åœ°æ”¯æŒ\p{L}ç­‰Unicodeå±æ€§
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
from functools import partial
from typing import Dict, List, Tuple, Optional, Iterable, Iterator

# --- æ ¸å¿ƒè¾…åŠ©å‡½æ•° ---

def get_pair_stats_optimized(word_freqs: Dict[Tuple[int, ...], int]) -> Dict[Tuple[int, int], int]:
    """
    ä»è¯é¢‘å­—å…¸ä¸­é«˜æ•ˆåœ°è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡ã€‚
    è¿™æ˜¯ä¼˜åŒ–çš„å…³é”®ï¼šæˆ‘ä»¬ä¸éå†æ•´ä¸ªæ–‡æœ¬ï¼Œè€Œæ˜¯éå†è¯æ±‡è¡¨å¹¶ä¹˜ä»¥å…¶é¢‘ç‡ã€‚
    """
    stats = {}
    for word, freq in word_freqs.items():
        for i in range(len(word) - 1):
            pair = (word[i], word[i+1])
            stats[pair] = stats.get(pair, 0) + freq
    return stats

def merge_word_freqs(word_freqs: Dict[Tuple[int, ...], int], pair: Tuple[int, int], new_id: int) -> Dict[Tuple[int, ...], int]:
    """
    åœ¨è¯é¢‘å­—å…¸çš„æ‰€æœ‰è¯ä¸­ï¼Œæ‰§è¡Œä¸€æ¬¡åˆå¹¶æ“ä½œã€‚
    è¿™ä¹Ÿæ˜¯ä¼˜åŒ–çš„å…³é”®ï¼šæˆ‘ä»¬ä¸ä¿®æ”¹ä¸€ä¸ªå·¨å¤§çš„åˆ—è¡¨ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªæ–°çš„è¯é¢‘å­—å…¸ã€‚
    """
    new_word_freqs = {}
    p1, p2 = pair
    for word, freq in word_freqs.items():
        new_word = []
        i = 0
        while i < len(word):
            if i < len(word) - 1 and word[i] == p1 and word[i+1] == p2:
                new_word.append(new_id)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        new_word_freqs[tuple(new_word)] = freq
    return new_word_freqs

def pretokenize_chunk(text_chunk: str, pat_str: str) -> Dict[Tuple[int, ...], int]:
    """å¹¶è¡ŒåŒ–é¢„åˆ†è¯çš„å·¥ä½œå‡½æ•°"""
    pat = regex.compile(pat_str)
    word_freqs = {}
    # ä½¿ç”¨ regex.findall è€Œä¸æ˜¯ re.findall
    for word_str in pat.findall(text_chunk):
        word_bytes = tuple(word_str.encode("utf-8"))
        word_freqs[word_bytes] = word_freqs.get(word_bytes, 0) + 1
    return word_freqs


# --- Problem 3: BPE è®­ç»ƒå‡½æ•° ---

def train_bpe(input_path: str, vocab_size: int, special_tokens: List[str]) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:
    """
    è®­ç»ƒä¸€ä¸ªå­—èŠ‚çº§çš„BPEåˆ†è¯å™¨ã€‚

    Args:
        input_path: è®­ç»ƒæ•°æ®è·¯å¾„ã€‚
        vocab_size: ç›®æ ‡è¯æ±‡è¡¨å¤§å°ã€‚
        special_tokens: ç‰¹æ®Štokenåˆ—è¡¨ã€‚

    Returns:
        A tuple containing:
            - vocab: ä»token IDåˆ°å…¶å­—èŠ‚åºåˆ—çš„æ˜ å°„ã€‚
            - merges: æŒ‰åˆ›å»ºé¡ºåºåˆ—å‡ºçš„BPEåˆå¹¶è§„åˆ™ã€‚
    """
    assert vocab_size >= 256 + len(special_tokens), "è¯æ±‡è¡¨å¤§å°ä¸è¶³"

    # 1. è¯æ±‡è¡¨åˆå§‹åŒ– (Vocabulary initialization)
    vocab = {i: bytes([i]) for i in range(256)}
    for i, token_str in enumerate(special_tokens):
        # å°†ç‰¹æ®Štokenæ”¾åœ¨è¯æ±‡è¡¨çš„æœ«å°¾ï¼ŒIDä»vocab_size-1å¼€å§‹é€’å‡
        # è¿™æ ·å¯ä»¥ç¡®ä¿å®ƒä»¬ä¸ä¼šä¸åˆå¹¶çš„token IDå†²çª
        token_id = vocab_size - 1 - i
        vocab[token_id] = token_str.encode("utf-8")

    # GPT-2çš„æ­£åˆ™è¡¨è¾¾å¼
    PAT_STR = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    
    # è¯»å–è¯­æ–™åº“
    with open(input_path, 'r', encoding='utf-8') as f:
        text = f.read()

    # 2. é¢„åˆ†è¯ (Pre-tokenization)
    # é¦–å…ˆæŒ‰ç‰¹æ®Štokenåˆ†å‰²è¯­æ–™åº“
    special_pattern = "|".join(map(re.escape, special_tokens))
    text_chunks = re.split(f"({special_pattern})", text)

    # å¹¶è¡ŒåŒ–é¢„åˆ†è¯
    num_procs = min(cpu_count(), os.cpu_count() or 1)
    with Pool(num_procs) as pool:
        # æˆ‘ä»¬åªå¯¹éç‰¹æ®Štokençš„å—è¿›è¡Œé¢„åˆ†è¯
        # å¥‡æ•°ç´¢å¼•çš„å—æ˜¯ç‰¹æ®Štokenæœ¬èº«ï¼Œå¶æ•°ç´¢å¼•æ˜¯æ™®é€šæ–‡æœ¬
        work_chunks = [chunk for i, chunk in enumerate(text_chunks) if i % 2 == 0 and chunk]
        
        # ä½¿ç”¨ partial æ¥ä¼ é€’å›ºå®šçš„ pat_str å‚æ•°ï¼ˆæˆ–è€…åƒä¸‹é¢è¿™æ ·ç”¨lambda/wrapperï¼‰
        # from functools import partial
        worker = partial(pretokenize_chunk, pat_str=PAT_STR)
        
        chunk_freqs_list = list(tqdm(
            pool.imap(worker, work_chunks),
            total=len(work_chunks),
            desc="å¹¶è¡Œé¢„åˆ†è¯"
        ))
    
    # åˆå¹¶æ‰€æœ‰è¿›ç¨‹çš„ç»“æœ
    word_freqs = {}
    for chunk_freqs in chunk_freqs_list:
        for word, freq in chunk_freqs.items():
            word_freqs[word] = word_freqs.get(word, 0) + freq

    # 3. è®¡ç®— BPE åˆå¹¶ (Compute BPE merges)
    num_merges = vocab_size - len(vocab)
    merges_list = []  # ä½¿ç”¨åˆ—è¡¨æ¥ä¿è¯é¡ºåº
    
    # è¯æ±‡è¡¨çš„IDä»256å¼€å§‹å¢é•¿
    next_token_id = 256
    
    pbar = tqdm(range(num_merges), desc="BPE åˆå¹¶")
    for i in pbar:
        # (a) ç»Ÿè®¡æ‰€æœ‰ç›¸é‚» token å¯¹çš„é¢‘ç‡
        stats = get_pair_stats_optimized(word_freqs)
        if not stats:
            print("æ²¡æœ‰æ›´å¤šçš„å¯¹å¯ä»¥åˆå¹¶ï¼Œæå‰åœæ­¢ã€‚")
            break

        # (b) æ‰¾åˆ°é¢‘ç‡æœ€é«˜çš„ token å¯¹ï¼Œå¹¶å¤„ç†å¹³å±€
        def tie_break_key(item):
            pair, freq = item
            # è¿”å›ä¸€ä¸ªå…ƒç»„ (é¢‘ç‡, å­—èŠ‚å¯¹)ï¼ŒPythonä¼šæŒ‰é¡ºåºæ¯”è¾ƒ
            return (freq, pair)
            
        best_pair = max(stats.items(), key=tie_break_key)[0]

        # (c) ç”¨ä¸€ä¸ªæ–°çš„ token "AB" æ›¿æ¢æ‰€æœ‰ ("A", "B") å¯¹
        word_freqs = merge_word_freqs(word_freqs, best_pair, next_token_id)

        # (d) å°† "AB" æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­
        p1_bytes = vocab[best_pair[0]]
        p2_bytes = vocab[best_pair[1]]
        vocab[next_token_id] = p1_bytes + p2_bytes

        # (e) å°† ("A", "B") è®°å½•åˆ°åˆå¹¶è§„åˆ™åˆ—è¡¨ merges ä¸­
        merges_list.append((p1_bytes, p2_bytes))
        pbar.set_description(f"åˆå¹¶ {p1_bytes.decode('utf-8', 'replace')}{p2_bytes.decode('utf-8', 'replace')} -> {next_token_id}")

        next_token_id += 1

    return vocab, merges_list


# --- Problem 5: Tokenizer ç±»å®ç° ---
# --- Problem 5: Tokenizer ç±»å®ç° (å·²ä¿®æ­£) ---

class BPE_Tokenizer:
    def __init__(self, vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], special_tokens: Optional[List[str]] = None):
        self.vocab = vocab
        # å°† merges è½¬ä¸ºå­—å…¸ï¼Œå€¼ä¸ºä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šä¼˜å…ˆï¼‰
        # é”®æ˜¯ (bytes, bytes)
        self.merges = {tuple(pair): i for i, pair in enumerate(merges)} 
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        PAT_STR = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
        self.pat = regex.compile(PAT_STR)

        # æ„å»ºç¼–ç å™¨ï¼šbytes -> ID
        self.encoder = {v: k for k, v in vocab.items()}
        # æ„å»ºè§£ç å™¨ï¼šID -> bytes
        self.decoder = vocab

        # å¤„ç†ç‰¹æ®Štoken
        self.special_tokens = set(special_tokens) if special_tokens else set()
        self.special_pattern = None
        self.special_encoder = {}
        if self.special_tokens:
            sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)
            pattern_str = "|".join(map(re.escape,sorted_special_tokens))
            self.special_pattern = regex.compile(f"({pattern_str})")
            # å»ºç«‹ç‰¹æ®Štokençš„å­—ç¬¦ä¸²åˆ°IDçš„æ˜ å°„
            for token_str in self.special_tokens:
                token_bytes = token_str.encode("utf-8")
                if token_bytes in self.encoder:
                    self.special_encoder[token_str] = self.encoder[token_bytes]

        # ç¼“å­˜
        self.cache = {}

    @classmethod
    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: Optional[List[str]] = None):
        with open(vocab_filepath, 'r', encoding='utf-8') as f:
            vocab_json = json.load(f)
            # å…³é”®ï¼šåŠ è½½ JSON æ—¶ï¼ŒKey æ˜¯å­—ç¬¦ä¸²ï¼Œå¿…é¡»è½¬ intï¼›Value æ˜¯å­—ç¬¦ä¸²ï¼Œå¿…é¡»è½¬ bytes
            # æ³¨æ„ï¼šè¿™é‡Œçš„ decode('unicode_escape').encode('latin1') æ˜¯ä¸ºäº†è¿˜åŸè¢« json åºåˆ—åŒ–æ—¶çš„å­—èŠ‚
            vocab = {}
            for k, v in vocab_json.items():
                vocab[int(k)] = v.encode('utf-8').decode('unicode_escape').encode('latin1')

        merges = []
        with open(merges_filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line: continue
                # mergesæ–‡ä»¶é€šå¸¸æ˜¯ "tokenA tokenB"
                parts = line.split()
                if len(parts) != 2: continue # è·³è¿‡ç©ºè¡Œæˆ–æ ¼å¼é”™è¯¯çš„è¡Œ
                p1, p2 = parts
                p1_bytes = p1.encode('utf-8').decode('unicode_escape').encode('latin1')
                p2_bytes = p2.encode('utf-8').decode('unicode_escape').encode('latin1')
                merges.append((p1_bytes, p2_bytes))
        
        return cls(vocab, merges, special_tokens)

    def save(self, vocab_filepath: str, merges_filepath: str):
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_json_save = {k: v.decode('latin1').encode('unicode_escape').decode('utf-8') for k, v in self.vocab.items()}
        with open(vocab_filepath, 'w', encoding='utf-8') as f:
            json.dump(vocab_json_save, f, ensure_ascii=False, indent=2)

        # ä¿å­˜åˆå¹¶è§„åˆ™
        with open(merges_filepath, 'w', encoding='utf-8') as f:
            for p1, p2 in self.merges.keys():
                 p1_str = p1.decode('latin1').encode('unicode_escape').decode('utf-8')
                 p2_str = p2.decode('latin1').encode('unicode_escape').decode('utf-8')
                 f.write(f"{p1_str} {p2_str}\n")
    
    def _bpe_merge(self, word_bytes: bytes) -> List[int]:
        """
        å¯¹å•ä¸ªå•è¯è¿›è¡ŒBPEåˆå¹¶ã€‚
        æ³¨æ„ï¼šè¿™é‡Œæ“ä½œçš„æ˜¯ Token IDï¼Œè€Œä¸æ˜¯åŸå§‹å­—èŠ‚å€¼ã€‚
        """
        if word_bytes in self.cache:
            return self.cache[word_bytes]

        # 1. åˆå§‹æ­¥éª¤ï¼šå°†å­—èŠ‚åºåˆ—è½¬æ¢ä¸º ID åºåˆ—
        # ä½ çš„æ—§ä»£ç ï¼štokens = list(word_bytes)  <-- é”™è¯¯ï¼šè¿™å‡è®¾äº† ID == ByteValue
        # ä¿®æ­£ä»£ç ï¼šæŸ¥ encoder è¡¨
        tokens = [self.encoder[bytes([b])] for b in word_bytes]

        while len(tokens) >= 2:
            # å¯»æ‰¾å½“å‰ tokens åˆ—è¡¨ä¸­æ‰€æœ‰ç›¸é‚»å¯¹ä¸­ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼ˆrankå€¼æœ€å°ï¼‰çš„ä¸€å¯¹
            stats = {}
            for i in range(len(tokens) - 1):
                # è·å–ç›¸é‚»ä¸¤ä¸ª ID å¯¹åº”çš„å­—èŠ‚åºåˆ—
                p1_bytes = self.decoder[tokens[i]]
                p2_bytes = self.decoder[tokens[i+1]]
                pair = (p1_bytes, p2_bytes)
                
                # å¦‚æœè¿™ä¸ªå¯¹åœ¨åˆå¹¶è§„åˆ™é‡Œï¼Œè®°å½•å®ƒçš„ä¼˜å…ˆçº§
                if pair in self.merges:
                    stats[pair] = self.merges[pair]

            # å¦‚æœæ²¡æœ‰å¯åˆå¹¶çš„å¯¹ï¼Œé€€å‡ºå¾ªç¯
            if not stats:
                break

            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜ï¼ˆæ•°å€¼æœ€å°ï¼‰çš„å¯¹
            best_pair = min(stats, key=stats.get)
            
            # è®¡ç®—åˆå¹¶åçš„æ–° Token çš„ ID
            # æ³¨æ„ï¼šåˆå¹¶åçš„ bytes = p1_bytes + p2_bytes
            merged_bytes = best_pair[0] + best_pair[1]
            new_id = self.encoder[merged_bytes]

            # æ‰§è¡Œåˆå¹¶ï¼šåœ¨ tokens åˆ—è¡¨ä¸­æ›¿æ¢æ‰æ‰€æœ‰çš„ best_pair
            new_tokens = []
            i = 0
            while i < len(tokens):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬è¦åˆå¹¶çš„å¯¹
                # éœ€è¦å†æ¬¡æŸ¥è¡¨ç¡®è®¤ current bytes æ˜¯å¦åŒ¹é…
                if i < len(tokens) - 1:
                    b1 = self.decoder[tokens[i]]
                    b2 = self.decoder[tokens[i+1]]
                    if (b1, b2) == best_pair:
                        new_tokens.append(new_id)
                        i += 2
                        continue
                
                new_tokens.append(tokens[i])
                i += 1
            
            tokens = new_tokens
        
        self.cache[word_bytes] = tokens
        return tokens

    def encode(self, text: str) -> List[int]:
        """å°†å­—ç¬¦ä¸²ç¼–ç ä¸º token ID åˆ—è¡¨"""
        token_ids = []
        
        # å¤„ç†ç‰¹æ®Š token
        if self.special_pattern:
            chunks = self.special_pattern.split(text)
            for i, chunk in enumerate(chunks):
                if i % 2 == 1: # ç‰¹æ®Š token
                    if chunk in self.special_encoder:
                        token_ids.append(self.special_encoder[chunk])
                    else:
                        # å¦‚æœç‰¹æ®Štokenä¸åœ¨è¡¨ä¸­ï¼Œå½“æ™®é€šæ–‡æœ¬å¤„ç†(å¾ˆå°‘å‘ç”Ÿ)
                        # æˆ–è€…åœ¨è¿™é‡ŒæŠ¥é”™
                        print(f"Warning: Special token {chunk} not found in vocab.")
                else: # æ™®é€šæ–‡æœ¬
                    if chunk:
                        for word in self.pat.findall(chunk):
                            word_bytes = word.encode("utf-8")
                            token_ids.extend(self._bpe_merge(word_bytes))
        else:
            for word in self.pat.findall(text):
                word_bytes = word.encode("utf-8")
                token_ids.extend(self._bpe_merge(word_bytes))

        return token_ids

    # --- æ–°å¢çš„æ–¹æ³•ï¼šProblem 6 è¦æ±‚ ---
    def encode_iterable(self, text_iterable: Iterable[str]) -> Iterator[int]:
        """
        å¯¹ä¸€ä¸ªæ–‡æœ¬è¿­ä»£å™¨è¿›è¡Œç¼–ç ã€‚
        è¿™ç”¨äºå¤„ç†å¤§å‹æ•°æ®é›†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ–‡æœ¬ã€‚
        """
        for text in text_iterable:
            yield from self.encode(text) #è¿”å›æ•´æ•°ID

    def decode(self, ids: List[int]) -> str:
        """å°† token ID åˆ—è¡¨è§£ç ä¸ºå­—ç¬¦ä¸²"""
        # æ³¨æ„ï¼šä½¿ç”¨ self.decoder æŠŠ ID è½¬å› bytes
        # errors='replace' é˜²æ­¢éæ³•çš„ UTF-8 åºåˆ—å¯¼è‡´å´©æºƒ
        all_bytes = b"".join(self.decoder[i] for i in ids)
        text = all_bytes.decode("utf-8", errors='replace')
        return text

        
# --- ä¸»æ‰§è¡Œå— (ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º) ---
if __name__ == '__main__':
    # --- Problem 4: åœ¨TinyStoriesä¸Šè®­ç»ƒ ---
    import time
    import resource

    # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„è®­ç»ƒæ–‡ä»¶
    # dummy_data_path = "TinyStoriesV2-GPT4-valid.txt"
    # with open(dummy_data_path, "w", encoding="utf-8") as f:
    #     f.write("low low low low low\n")
    #     f.write("lower lower widest widest widest\n")
    #     f.write("newest newest newest newest newest newest\n")
    #     f.write("This is a simple test for the BPE tokenizer. It should handle Unicode like ğŸ˜Š and CJK characters like ä½ å¥½ä¸–ç•Œã€‚\n")
    INPUT_PATH = "train1.txt"
    # è®­ç»ƒå‚æ•°
    VOCAB_SIZE = 5000
    SPECIAL_TOKENS = ["<|endoftext|>"]
  

    # (a) è®­ç»ƒåˆ†è¯å™¨
    print("å¼€å§‹è®­ç»ƒBPEåˆ†è¯å™¨...")
    start_time = time.time()
    
    vocab, merges = train_bpe(INPUT_PATH, VOCAB_SIZE, SPECIAL_TOKENS)
    
    end_time = time.time()
    training_time = end_time - start_time
    
    # æŠ¥å‘Šè®­ç»ƒæ—¶é—´å’Œå†…å­˜å ç”¨
    memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024  # in MB
    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    print(f"å³°å€¼å†…å­˜å ç”¨: {memory_usage:.2f} MB")

    # è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token
    longest_token = max(vocab.values(), key=len)
    print(f"è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token (bytes): {longest_token}")
    print(f"è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token (str): '{longest_token.decode('utf-8', 'replace')}'")

    # ä¿å­˜è®­ç»ƒç»“æœ
    VOCAB_FILE = "tinystories_vocab.json"
    MERGES_FILE = "tinystories_merges.txt"
    tokenizer_for_saving = Tokenizer(vocab, merges, SPECIAL_TOKENS)
    tokenizer_for_saving.save(VOCAB_FILE, MERGES_FILE)
    print(f"è¯æ±‡è¡¨å·²ä¿å­˜åˆ° {VOCAB_FILE}")
    print(f"åˆå¹¶è§„åˆ™å·²ä¿å­˜åˆ° {MERGES_FILE}")

    # (b) æ€§èƒ½åˆ†ææç¤º:
    print("\n(b) æ€§èƒ½åˆ†ææç¤º:")
    print("è¦å¯¹ä»£ç è¿›è¡Œæ€§èƒ½åˆ†æï¼Œå¯ä»¥ä½¿ç”¨cProfileæ¨¡å—ã€‚")
    print("ä¾‹å¦‚: python -m cProfile -o profile.stats your_script_name.py")
    print("ç„¶åä½¿ç”¨ `snakeviz profile.stats` æ¥å¯è§†åŒ–ç»“æœã€‚")
    print("é¢„è®¡ `get_pair_stats_optimized` å’Œ `merge_word_freqs` ä¼šæ˜¯åˆå¹¶æ­¥éª¤ä¸­çš„çƒ­ç‚¹ã€‚")
    
    # --- Problem 5 & 6: ä½¿ç”¨Tokenizerç±» ---
    print("\n--- Tokenizer å®éªŒ ---")
    
    # ä»æ–‡ä»¶åŠ è½½åˆ†è¯å™¨
    tokenizer = Tokenizer.from_files(VOCAB_FILE, MERGES_FILE, SPECIAL_TOKENS)
    
    # æµ‹è¯•ç¼–ç å’Œè§£ç 
    text_to_test = "newest low lower ğŸ˜Šä½ å¥½<|endoftext|>"
    encoded = tokenizer.encode(text_to_test)
    decoded = tokenizer.decode(encoded)
    
    print(f"åŸå§‹æ–‡æœ¬: '{text_to_test}'")
    print(f"ç¼–ç ç»“æœ (token IDs): {encoded}")
    print(f"è§£ç ç»“æœ: '{decoded}'")
    assert text_to_test == decoded
    print("ç¼–ç  -> è§£ç  ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡ï¼")

    # (a) è®¡ç®—å‹ç¼©ç‡
    sample_text = "This is a sample document from TinyStories dataset to calculate the compression ratio."
    encoded_sample = tokenizer.encode(sample_text)
    num_bytes = len(sample_text.encode("utf-8"))
    num_tokens = len(encoded_sample)
    compression_ratio = num_bytes / num_tokens
    print(f"\n(a) å‹ç¼©ç‡ (bytes/token): {compression_ratio:.2f} ({num_bytes} bytes / {num_tokens} tokens)")

    # (b) ä¼°ç®—ååé‡
    large_text = sample_text * 1000
    start_time_enc = time.time()
    tokenizer.encode(large_text)
    end_time_enc = time.time()
    duration_enc = end_time_enc - start_time_enc
    throughput = len(large_text.encode("utf-8")) / duration_enc / 1e6 # MB/s
    print(f"(b) ç¼–ç ååé‡: {throughput:.2f} MB/s")

    # (c) ä¸ºä»€ä¹ˆ uint16 æ˜¯åˆé€‚çš„æ•°æ®ç±»å‹ï¼Ÿ
    print("\n(c) ä¸ºä»€ä¹ˆ uint16 æ˜¯åˆé€‚çš„æ•°æ®ç±»å‹ï¼Ÿ")
    print(f"ä¸€ä¸ªæ— ç¬¦å·16ä½æ•´æ•° (uint16) å¯ä»¥è¡¨ç¤º 2^16 = 65,536 ä¸ªä¸åŒçš„å€¼ (ä» 0 åˆ° 65,535)ã€‚")
    print(f"å¯¹äº 5K, 10K, æˆ– 32K å¤§å°çš„è¯æ±‡è¡¨ï¼Œè¿™ä¸ªèŒƒå›´å®Œå…¨è¶³å¤Ÿè¦†ç›–æ‰€æœ‰çš„token IDã€‚")
    print("ç›¸æ¯”ä½¿ç”¨é»˜è®¤çš„int32æˆ–int64ï¼Œä½¿ç”¨uint16å¯ä»¥èŠ‚çœä¸€åŠæˆ–æ›´å¤šçš„å†…å­˜/ç£ç›˜ç©ºé—´ï¼Œè¿™åœ¨å¤„ç†æ•°åäº¿çº§åˆ«çš„tokenæ—¶éå¸¸é‡è¦ã€‚")
